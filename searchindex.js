Search.setIndex({"docnames": ["advanced", "bardi.data", "bardi.nlp_engineering", "bardi.pipeline", "basic", "index", "installation"], "filenames": ["advanced.rst", "bardi.data.rst", "bardi.nlp_engineering.rst", "bardi.pipeline.rst", "basic.rst", "index.rst", "installation.rst"], "titles": ["Advanced Tutorials", "bardi.data package", "bardi.nlp_engineering package", "bardi.pipeline", "Basic Tutorial", "Welcome to bardi\u2019s documentation!", "Installation"], "terms": {"note": [0, 2, 4], "all": [0, 1, 2, 3, 4], "code": [0, 2, 4], "snippet": 0, "thi": [0, 1, 2, 3, 4], "page": 0, "ar": [0, 1, 2, 3, 4, 5], "base": [0, 1, 2, 3, 5], "setup": 0, "perform": [0, 1, 2, 3, 4], "basic": [0, 5], "As": 0, "from": [0, 1, 2, 3, 4], "mai": [0, 2, 4], "run": [0, 2, 3, 5], "exactli": [0, 2], "i": [0, 1, 2, 3, 4, 5], "without": 0, "ad": [0, 3, 5], "some": [0, 2, 3, 4], "dataset": [0, 1, 2, 3, 5], "document": [0, 1, 4], "refer": [0, 1, 4], "bardi": [0, 6], "nlp_engin": [0, 4, 5], "cpunorm": [0, 2, 4], "regex_librari": [0, 2, 4], "pathology_report": [0, 2, 4], "pathologyreportregexset": [0, 2, 4], "The": [0, 1, 2, 4], "need": [0, 2, 3, 4, 5], "regular": [0, 2, 4], "express": [0, 2, 4], "set": [0, 1, 3, 5], "regex_set": [0, 4, 5], "appli": [0, 2, 4], "text": [0, 2, 4], "A": [0, 1, 2, 3, 4], "just": [0, 2, 4], "python": 0, "list": [0, 1, 2, 3, 4], "dictionari": [0, 2, 3], "contain": [0, 1, 2], "kei": [0, 2, 3, 4, 5], "regex_str": [0, 2, 4], "substitut": [0, 2, 4], "string": [0, 2], "sub_str": [0, 2, 4], "There": [0, 4], "few": 0, "method": [0, 2, 3, 4], "you": [0, 1, 2, 4], "can": [0, 1, 2, 3, 4], "us": [0, 1, 2, 3, 4, 5], "what": 0, "your": [0, 4], "tune": 0, "provid": [0, 3, 4, 5], "regexset": [0, 2], "an": [0, 1, 2, 5], "exist": [0, 1, 2, 3], "tunabl": 0, "turn": 0, "individu": [0, 4], "off": 0, "desir": [0, 2, 3, 4], "By": [0, 2, 5], "default": [0, 2, 3], "so": [0, 2, 4], "would": [0, 2, 4], "ones": 0, "don": [0, 4], "t": [0, 2, 4], "want": [0, 1, 2, 4], "import": [0, 2, 4], "grab": [0, 4], "pre": [0, 2, 3, 5], "made": [0, 2, 4], "regex": [0, 4, 5], "pathologi": [0, 2, 4], "report": [0, 2, 4], "3": [0, 2, 4], "path_report_regex_set": [0, 4], "get_regex_set": [0, 2, 4], "remove_dimens": [0, 2], "fals": [0, 2, 3, 4], "remove_specimen": [0, 2], "remove_decimal_seg_numb": [0, 2], "append": 0, "suppos": 0, "have": [0, 2, 4], "addit": [0, 1, 2, 4, 5], "like": [0, 2], "add": [0, 2, 3, 4], "becaus": [0, 2, 4], "return": [0, 1, 2, 3], "pair": [0, 1, 2], "0": [0, 2, 4], "9": [0, 2, 4], "moneytoken": 0, "built": [0, 4], "type": [0, 1, 2, 3], "write": [0, 1, 2, 3], "noth": [0, 4], "stop": 0, "own": 0, "custom_regex_set": 0, "": [0, 1, 2, 3, 4], "http": [0, 2, 4], "urltoken": [0, 2, 4], "suppli": [0, 1, 2, 3, 4], "add_step": [0, 3, 4], "nlp": [0, 4], "field": [0, 2, 4], "lowercas": [0, 2, 4], "true": [0, 2, 3, 4], "class": [0, 1, 2, 3, 4], "howev": [0, 2, 4], "more": [0, 2, 4], "organ": [0, 3], "approach": 0, "than": [0, 2], "shown": 0, "abov": [0, 4], "handl": [0, 1, 2, 4], "could": [0, 4], "follow": [0, 2, 4, 5], "our": [0, 5], "lead": [0, 2], "new": [0, 2, 3, 5], "inherit": 0, "give": 0, "attribut": [0, 1, 2, 3], "which": [0, 1, 2, 3], "mycustomregexset": 0, "def": 0, "__init__": 0, "self": 0, "handle_whitespac": [0, 2], "bool": [0, 1, 2, 3], "convert_url": 0, "we": [0, 2, 4], "out": [0, 4], "box": 0, "hope": 0, "continu": [0, 4], "help": [0, 2], "expect": [0, 2, 3], "everi": 0, "possibl": [0, 1, 4], "data": [0, 2, 3, 5], "process": [0, 2, 3, 5], "action": 0, "ever": 0, "someth": [0, 2], "still": 0, "framework": [0, 3, 4, 5], "guidelin": 0, "within": [0, 2, 4, 5], "alongsid": 0, "ani": [0, 2, 3, 4], "metadata": [0, 5], "captur": 0, "standard": [0, 2], "file": [0, 1, 2, 3, 4], "reproduc": [0, 1, 2, 4, 5], "let": [0, 4], "look": [0, 4], "full": [0, 5], "exampl": [0, 2, 4], "ll": 0, "explain": 0, "happen": 0, "tupl": [0, 2, 3], "option": [0, 2], "panda": [0, 1, 4], "pd": [0, 4], "polar": [0, 1, 5], "pl": 0, "pyarrow": [0, 1, 2, 3, 4], "pa": [0, 2, 3, 4], "bardi_data": [0, 4], "sampl": [0, 2, 5], "df": [0, 1, 4], "datafram": [0, 1, 4], "patient_id_numb": [0, 2, 4], "1": [0, 2, 4], "patient": [0, 2, 4], "present": [0, 1, 2, 4], "notabl": [0, 4], "chang": [0, 4], "behavior": [0, 3, 4], "exhibit": [0, 4], "increas": [0, 4], "aggress": [0, 4], "impuls": [0, 4], "distinct": [0, 4], "deviat": [0, 4], "jedi": [0, 4], "preliminari": [0, 4], "examin": [0, 4], "reveal": [0, 4], "heighten": [0, 4], "midichlorian": [0, 4], "count": [0, 1, 4], "unsettl": [0, 4], "connect": [0, 4], "dark": [0, 4], "side": [0, 4], "forc": [0, 2, 4], "further": [0, 2, 4], "analysi": [0, 4], "warrant": [0, 4], "explor": [0, 4], "extent": [0, 4], "exposur": [0, 4], "sith": [0, 4], "teach": [0, 4], "It": [0, 2, 4, 5], "imper": [0, 4], "monitor": [0, 4], "close": [0, 4], "worsen": [0, 4], "symptom": [0, 4], "engag": [0, 4], "therapeut": [0, 4], "intervent": [0, 4], "aim": [0, 4], "prevent": [0, 2, 4], "descent": [0, 4], "up": [0, 2, 4], "assess": [0, 4], "crucial": [0, 2, 4], "determin": [0, 4], "efficaci": [0, 4], "strategi": [0, 4], "overal": [0, 4], "trajectori": [0, 4], "align": [0, 1, 4], "dark_side_dx": [0, 4], "posit": [0, 2, 4], "2": [0, 2, 4], "sign": [0, 2, 4], "succumb": [0, 4], "indic": [0, 2, 4], "stabl": [0, 4], "commit": [0, 4], "No": [0, 2, 4], "influenc": [0, 4], "were": [0, 4], "observ": [0, 4], "check": [0, 4], "ins": [0, 4], "council": [0, 4], "ensur": [0, 2, 4], "sustain": [0, 4], "well": [0, 4], "being": [0, 3, 4], "order": [0, 2, 4], "neg": [0, 2, 4], "manifest": [0, 4], "palpabl": [0, 4], "establish": [0, 4], "ethic": [0, 4], "initi": [0, 5], "disclos": [0, 4], "elev": [0, 4], "unmistak": [0, 4], "investig": [0, 4], "ascertain": [0, 4], "depth": [0, 4], "doctrin": [0, 4], "essenti": [0, 4], "track": [0, 4], "exacerb": [0, 4], "advis": [0, 4], "forestal": [0, 4], "deeper": [0, 4], "embrac": [0, 4], "subsequ": [0, 4], "evalu": [0, 4], "pivot": [0, 4], "gaug": [0, 4], "effect": [0, 2, 4], "allegi": [0, 4], "regist": [0, 5], "from_panda": [0, 1, 4, 5], "write_output": [0, 3, 4], "pathology_regex_set": [0, 4], "token": [0, 2, 4], "cpupretoken": [0, 2, 4], "split_pattern": [0, 2, 4], "length": [0, 2], "each": [0, 2, 3], "record": [0, 1, 2, 4], "mycustomstep": 0, "str": [0, 1, 2, 3, 4], "super": 0, "tabl": [0, 1, 2, 3, 4], "artifact": [0, 2, 3, 4], "dict": [0, 1, 2, 3], "none": [0, 1, 2, 3, 4], "from_arrow": 0, "with_column": 0, "col": 0, "alia": 0, "token_count": 0, "to_arrow": 0, "run_pipelin": [0, 3, 4], "In": [0, 2, 4], "coupl": 0, "These": [0, 5], "clean": [0, 2, 4], "split": [0, 1, 2, 4], "space": [0, 2], "next": 0, "do": [0, 4], "constructor": 0, "take": 0, "input": [0, 1, 2], "know": 0, "column": [0, 1, 2, 4], "user": [0, 5], "If": [0, 1, 2, 3, 4], "see": 0, "accept": [0, 1], "two": [0, 2], "paramet": [0, 1, 2, 3], "pass": [0, 1, 2, 3], "variou": [0, 1], "transform": [0, 2, 3, 5], "work": [0, 4], "ignor": [0, 2, 3], "argument": [0, 1], "correctli": [0, 2], "similarli": 0, "first": [0, 2], "element": [0, 2], "second": [0, 2], "produc": [0, 2, 3, 4], "isn": 0, "alter": 0, "place": 0, "aren": 0, "show": [0, 4], "didn": 0, "inbetween": 0, "output": [0, 2, 3], "total": [0, 1, 2, 4], "arrow": [0, 1, 5], "highli": 0, "rest": 0, "how": [0, 2, 4], "instanc": 0, "oper": [0, 1, 2, 3, 4, 5], "convert": [0, 1, 2, 4], "back": [0, 2], "after": [0, 1, 2], "defin": [0, 2, 3], "ran": 0, "ha": 0, "automat": [0, 4], "includ": [0, 2], "get": 0, "implement": [0, 2, 3, 4], "probabl": [0, 1], "good": 0, "enough": 0, "most": 0, "definit": 1, "handler": [1, 3], "function": [1, 4, 5], "load": [1, 2, 4], "sourc": [1, 2, 3], "object": [1, 2, 3, 4], "form": [1, 2], "row": 1, "under": [1, 2], "hood": 1, "modern": 1, "effici": [1, 5], "start": [1, 2], "point": 1, "both": [1, 5], "cpu": [1, 2, 5], "gpu": 1, "workflow": [1, 4, 5], "origin_queri": [1, 4], "sql": 1, "wa": [1, 2], "queri": 1, "here": [1, 2, 4], "proven": 1, "origin_file_path": 1, "filepath": 1, "origin_format": [1, 4], "format": [1, 2], "origin_row_count": [1, 4], "origin": [1, 2], "int": [1, 2, 4], "get_paramet": [1, 2, 3, 4], "from_duckdb": [1, 5], "path": [1, 2], "min_batch": 1, "creat": [1, 2, 3, 4, 5], "custom": [1, 2, 3, 4, 5], "duckdb": 1, "databas": 1, "valid": [1, 2], "adher": 1, "syntax": 1, "specif": [1, 2, 3, 5], "integ": [1, 2], "number": [1, 2], "amount": 1, "smaller": [1, 2], "distribut": [1, 2], "worker": 1, "node": 1, "referenc": [1, 2], "convers": 1, "from_fil": [1, 5], "arg": [1, 2], "kwarg": [1, 2], "parquet": [1, 3, 4], "ipc": 1, "feather": 1, "csv": [1, 3], "orc": 1, "util": [1, 5], "api": [1, 3], "read": [1, 2], "thu": 1, "keyword": 1, "avail": [1, 2, 3, 4, 5], "its": [1, 2, 5], "singl": [1, 2, 4], "current": [1, 2, 6], "support": [1, 2, 6], "comput": [1, 2, 4, 5], "environ": 1, "rais": [1, 2], "valueerror": 1, "doe": [1, 2, 4], "filetyp": [1, 3], "from_json": [1, 5], "json_data": 1, "union": [1, 2, 3], "json": [1, 4], "name": [1, 2, 3, 4], "valu": [1, 2, 3], "becom": [1, 2], "intend": [1, 2], "pipelin": [1, 2, 5], "flag": 1, "prompt": 1, "chunk": 1, "prepar": [1, 5], "later": 1, "also": [1, 2, 3, 4], "direct": [1, 2], "futur": [1, 5], "manner": 1, "from_pyarrow": [1, 5], "to_panda": [1, 4, 5], "same": [1, 2], "to_polar": [1, 5], "write_fil": [1, 5], "onli": [1, 2, 3, 4, 6], "subset": 1, "filesystem": 1, "where": [1, 2, 3, 4], "written": [1, 2, 3], "npy": [1, 4], "regexsubpair": 2, "dure": 2, "retain_input_field": 2, "retain": 2, "content": 2, "specifi": [2, 3, 4], "normalizer__": 2, "configur": [2, 3], "least": 2, "one": [2, 4], "large_str": 2, "must": 2, "receiv": [2, 3], "empti": 2, "avoid": 2, "instanti": 2, "instead": 2, "child": 2, "depend": 2, "hardwar": 2, "abstract": [2, 3, 4, 5], "simpl": [2, 4], "pattern": [2, 3, 4], "pretoken": [2, 5], "break": 2, "down": 2, "unit": [2, 5], "befor": 2, "charact": 2, "divid": 2, "segment": 2, "done": 2, "pretokenizer__": 2, "train": [2, 4], "model": [2, 4, 5], "cputokenizertrain": 2, "tokenizertrain": 2, "transformertoken": 2, "tokenizer_typ": 2, "scratch": 2, "wordpiec": 2, "bpe": 2, "unigram": 2, "wordlevel": 2, "vocab_s": [2, 4], "hf_cache_dir": 2, "folder": 2, "hf": 2, "store": 2, "from_old_flag": 2, "templat": 2, "checkpoint_path": 2, "pretrain": 2, "tokenizer_fnam": 2, "corpus_gen_batch_s": 2, "size": 2, "batch": [2, 5], "corpu": 2, "deafult": 2, "1000": 2, "retriv": 2, "embed": [2, 4], "generetor": 2, "represent": 2, "trainer": 2, "cputransformertoken": 2, "consum": 2, "embedding_matrix": [2, 4], "id_to_token": [2, 4], "write_artifact": [2, 3], "write_path": [2, 3], "oartifactsproduc": 2, "directori": [2, 3], "special_token": 2, "abil": 2, "old": 2, "ONE": 2, "associ": 2, "given": 2, "architectur": [2, 5], "bert": 2, "llama": 2, "agnost": 2, "algorithm": 2, "set_write_config": [2, 3], "data_config": [2, 3], "datawriteconfig": [2, 3, 5], "artifacts_config": 2, "tokenizertrainerartifactswriteconfig": 2, "overwrit": [2, 3], "typeddict": [2, 3], "config": 2, "gener": [2, 4], "vocab_format": [2, 4], "vocab_format_arg": [2, 4], "word2vec": [2, 4], "vocab": [2, 4], "word": [2, 4], "cpuembeddinggener": [2, 4], "embeddinggener": [2, 5], "interfac": 2, "vector": 2, "gensim": [2, 4], "consid": 2, "load_saved_model": 2, "checkpoint": 2, "core": [2, 4], "min_word_count": [2, 4], "frequenc": 2, "lower": 2, "window": [2, 4], "maximum": 2, "distanc": 2, "between": 2, "predict": 2, "vector_s": [2, 4], "threshold": 2, "high": 2, "randomli": 2, "downsampl": 2, "rang": 2, "1e": 2, "5": [2, 4], "float": 2, "min_alpha": [2, 4], "learn": [2, 5], "rate": 2, "linearli": 2, "drop": 2, "progress": 2, "epoch": [2, 4], "iter": 2, "seed": [2, 4], "random": 2, "For": [2, 4], "determinist": 2, "thread": 2, "aka": [2, 4], "pythonhashse": 2, "vocab_exclude_list": [2, 4], "exclud": [2, 3], "retriev": 2, "4": [2, 4], "10": [2, 4], "300": [2, 4], "6e": [2, 4], "05": [2, 4], "007": [2, 4], "20": [2, 4], "30": [2, 4], "42": [2, 4], "whether": 2, "save": [2, 3, 4], "sentenc": 2, "dimension": 2, "higher": 2, "mani": [2, 4], "nois": 2, "should": [2, 3], "drawn": 2, "over": 2, "vocabulari": 2, "embeddinggeneratorartifactswriteconfig": 2, "embedding_matrix_format": [2, 4], "embedding_matrix_format_arg": [2, 4], "map": [2, 4], "cpuvocabencod": [2, 4], "vocabencod": [2, 5], "encod": [2, 4], "field_renam": [2, 4], "renam": [2, 4], "id": 2, "construct": 2, "altern": 2, "flexibl": [2, 5], "versu": 2, "concat_field": 2, "concaten": 2, "left": 2, "separ": 2, "vocabencoder__": 2, "larg": [2, 3], "reli": 2, "multipl": [2, 4], "wai": [2, 4], "creation": [2, 4], "wasn": 2, "through": [2, 5], "final": [2, 3, 4], "hold": 2, "attributeerror": 2, "either": 2, "typeerror": 2, "cputokenizerencod": 2, "tokenizerencod": 2, "return_tensor": 2, "tensor": 2, "np": 2, "numpi": 2, "arrai": 2, "pt": 2, "pytorch": 2, "tf": 2, "tensorflow": 2, "cputokenizerencoder_input__": 2, "retain_concat_field": 2, "local": 2, "model_nam": 2, "Not": 2, "requir": [2, 3, 4, 5], "prior": 2, "multithread": [2, 5], "tokenizer_param": 2, "fine": 2, "grain": 2, "tokenizerconfig": 2, "tokenizer_model": 2, "pretrainedtokenizerbas": 2, "post": 2, "processor": [2, 4], "tokenizerencoder__": 2, "huggingfac": 2, "label": [2, 4], "numer": 2, "cpulabelprocessor": [2, 4], "labelprocessor": [2, 5], "uniqu": [2, 4, 5], "id_to_label": [2, 4], "revers": 2, "Of": 2, "downstream": 2, "labelprocessor__": 2, "alreadi": 2, "directli": 2, "notimplementederror": 2, "other": 2, "respect": [2, 3], "labelprocessorartifactswriteconfig": 2, "id_to_label_arg": 2, "id_to_label_format": [2, 4], "test": [2, 4], "val": [2, 4], "cpusplitt": 2, "assign": 2, "particular": [2, 3], "previous": 2, "when": [2, 4], "comparison": 2, "To": 2, "appropri": 2, "namedtupl": 2, "e": [2, 3], "split_method": 2, "newsplit": [2, 4], "split_proport": 2, "75": 2, "15": [2, 4], "unique_record_col": 2, "document_id": 2, "group_col": 2, "registri": 2, "labels_col": 2, "random_se": 2, "mapsplit": 2, "differ": 2, "upon": 2, "split_typ": 2, "combin": 2, "identifi": 2, "oftentim": 2, "program": 2, "crash": 2, "split_map": 2, "hash": 2, "repres": 2, "pseudocod": 2, "concat": 2, "correspond": [2, 3], "fold1": 2, "fold2": 2, "fold3": 2, "etc": [2, 3], "proport": 2, "25": [2, 4], "fold4": 2, "num_split": 2, "group": 2, "keep": [2, 4], "discret": 2, "x": [2, 4], "had": [2, 4], "medic": 2, "end": [2, 4], "potenti": 2, "inform": 2, "leakag": 2, "case": [2, 4], "patient_id": 2, "label_col": 2, "effort": 2, "balanc": 2, "across": 2, "guarante": 2, "prefer": 2, "try": 2, "default_split_valu": 2, "line": [2, 6], "cannot": 2, "found": 2, "blueprint": [2, 3], "domain": 2, "lowercase_substitut": 2, "no_substitut": 2, "tokeen": 2, "datetoken": [2, 4], "remov": [2, 3], "match": 2, "replac": [2, 3], "special": [2, 5], "optionl": 2, "sub": 2, "r": [2, 4], "whitespac": 2, "curat": 2, "convert_escape_cod": 2, "remove_url": 2, "remove_special_punct": 2, "remove_multiple_punct": 2, "handle_angle_bracket": 2, "replace_percent_sign": 2, "handle_leading_digit_punct": 2, "remove_leading_punct": 2, "remove_trailing_punct": 2, "handle_words_with_punct_spac": 2, "handle_math_spac": 2, "handle_dimension_spac": 2, "handle_measure_spac": 2, "handle_cassettes_spac": 2, "handle_dash_digit_spac": 2, "handle_literals_floats_spac": 2, "fix_plur": 2, "handle_digits_words_spac": 2, "remove_phone_numb": 2, "remove_d": 2, "remove_tim": 2, "remove_address": 2, "remove_large_digits_seq": 2, "remove_large_floats_seq": 2, "trunc_decim": 2, "remove_cassette_nam": 2, "remove_duration_tim": 2, "remove_letter_num_seq": 2, "tailor": [2, 5], "craft": [2, 5], "understand": 2, "punctuat": 2, "often": 2, "result": [2, 5], "loss": 2, "g": 2, "term": 2, "her": 2, "remain": 2, "manag": 2, "emploi": 2, "around": 2, "22": [2, 4], "year": 2, "consider": 2, "particularli": [2, 4], "excess": 2, "imped": 2, "dilut": 2, "concept": 2, "escap": 2, "x0d": 2, "x0a": 2, "extra": 2, "carriag": 2, "tab": 2, "url": 2, "duplic": 2, "angl": 2, "bracket": 2, "titl": 2, "percent": [2, 4], "digit": 2, "attach": 2, "trail": 2, "hyphen": 2, "colon": 2, "period": 2, "them": [2, 4], "math": 2, "symbol": 2, "aroud": 2, "measur": 2, "mm": 2, "cm": 2, "ml": [2, 4], "proper": 2, "5e": 2, "6f": 2, "dash": 2, "common": [2, 4, 5], "problem": 2, "r18": 2, "0admiss": 2, "admiss": 2, "restor": 2, "plural": 2, "noun": 2, "demag": 2, "begin": 2, "phone": 2, "consist": [2, 3], "delimet": 2, "date": [2, 4], "prespecifi": 2, "time": [2, 4], "11": [2, 4], "am": 2, "30pm": 2, "52": 2, "07am": 2, "address": 2, "num": 2, "street": 2, "6": [2, 4], "letter": 2, "state": 2, "short": 2, "long": 2, "zip": 2, "2d": 2, "3d": 2, "dimens": 2, "mark": 2, "speciman": 2, "78": 2, "87": 2, "sequenc": 2, "decim": 2, "durat": 2, "speciment": 2, "treat": 2, "32d09090301": 2, "regex_lib": 2, "get_address_regex": 2, "1034": 2, "north": 2, "500": 2, "west": 2, "provo": 2, "ut": 2, "84604": 2, "3337": 2, "addresstoken": [2, 4], "get_angle_brackets_regex": 2, "fix": 2, "But": 2, "90": 2, "get_cassette_name_regex": 2, "cassett": 2, "block": 2, "cassettetoken": [2, 4], "get_cassettes_spacing_regex": 2, "3e": 2, "3f": 2, "get_dash_digits_spacing_regex": 2, "right": 2, "7": [2, 4], "get_dates_regex": 2, "co": 2, "03": [2, 4], "09": 2, "2021": 2, "1015": 2, "complet": 2, "21": [2, 4], "34": [2, 4], "get_decimal_segmented_numbers_regex": 2, "decimalsegmentednumbertoken": [2, 4], "get_digits_words_spacing_regex": 2, "9837648admiss": 2, "9837648": 2, "get_dimension_spacing_regex": 2, "3x0": 2, "7x0": 2, "get_dimensions_regex": 2, "33": [2, 4], "dimensiontoken": [2, 4], "get_duration_regex": 2, "specimen": 2, "32d0909091": 2, "durationtoken": [2, 4], "get_escape_code_regex": 2, "r30": 2, "get_fix_pluralization_regex": 2, "get_large_digits_seq_regex": 2, "456123456": 2, "digitsequencetoken": [2, 4], "get_large_float_seq_regex": 2, "456": 2, "123456": 2, "783": 2, "largefloattoken": [2, 4], "get_leading_digit_punctuation_regex": 2, "proce": 2, "elimin": 2, "insert": 2, "13": [2, 4], "unremark": 2, "get_leading_punctuation_regex": 2, "3a": 2, "anterior": 2, "get_letter_num_seq_regex": 2, "c001234567": 2, "letterdigitstoken": [2, 4], "get_literals_floats_spacing_regex": 2, "diagnosi": 2, "bi": 2, "n13": 2, "30admiss": 2, "get_math_spacing_regex": 2, "95": [2, 4], "8": [2, 4], "get_measure_spacing_regex": 2, "spece": 2, "11th": 2, "th": 2, "10mm": 2, "histolog": 2, "3cm": 2, "get_multiple_punct_regex": 2, "occur": 2, "_": [2, 4], "___": 2, "signatur": 2, "get_percent_sign_regex": 2, "strong": 2, "intens": 2, "get_phone_number_regex": 2, "ph": 2, "123": 2, "7890": 2, "4567890": 2, "phonenumtoken": [2, 4], "get_spaces_regex": 2, "locat": 2, "arm": 2, "get_special_punct_regex": 2, "chosen": 2, "wt": 2, "ck": 2, "focal": 2, "sth": 2, "ab": 2, "cd": 2, "get_specimen_regex": 2, "009345": 2, "sh": 2, "0011300": 2, "specimentoken": [2, 4], "get_time_regex": 2, "12": [2, 4], "pm": 2, "12am": 2, "timetoken": [2, 4], "get_trailing_punctuation_regex": 2, "get_trunc_decimals_regex": 2, "99": 2, "get_urls_regex": 2, "www": [2, 4], "merck": 2, "com": 2, "keytruda_pi": 2, "pdf": 2, "get_whitespace_regex": 2, "invas": 2, "nneg": 2, "IN": 2, "situ": 2, "nn": 2, "tthe": 2, "n": [2, 4], "get_words_with_punct_spacing_regex": 2, "d": [2, 4], "tiff": 2, "1k": 2, "descript": 2, "gleason": 2, "step": [3, 4, 5], "data_format": [3, 4], "data_format_arg": [3, 4], "liter": 3, "debug": 3, "data_write_config": 3, "data_filenam": 3, "bardi_processed_data": 3, "data_handl": [3, 4, 5], "system": 3, "filenam": 3, "execut": 3, "condens": 3, "call": 3, "item": 3, "copi": 3, "preced": 3, "processed_data": [3, 4], "besid": 3, "sinc": [3, 4], "write_data": 3, "reus": 3, "extens": [3, 5], "offer": [4, 5], "sever": 4, "now": 4, "go": 4, "link": 4, "doesn": 4, "involv": 4, "splitter": [4, 5], "pleas": 4, "Then": 4, "pre_token": [4, 5], "pretti": 4, "embedding_gener": [4, 5], "fair": 4, "warn": 4, "far": 4, "slowest": 4, "part": 4, "routin": 4, "account": 4, "about": 4, "control": 4, "aspect": [4, 5], "simpli": 4, "small": 4, "vocab_encod": [4, 5], "label_processor": [4, 5], "again": 4, "straight": 4, "forward": 4, "actual": 4, "too": 4, "final_data": 4, "label_map": 4, "word_embed": 4, "39": 4, "45": 4, "44": 4, "23": 4, "31": 4, "41": 4, "35": 4, "24": 4, "18": 4, "pad": 4, "14": 4, "16": 4, "17": 4, "19": 4, "26": 4, "27": 4, "28": 4, "29": 4, "32": 4, "36": 4, "37": 4, "38": 4, "40": 4, "43": 4, "unk": 4, "matrix": 4, "00000000e": 4, "00": 4, "77135365e": 4, "86092880e": 4, "04": 4, "89334818e": 4, "73368554e": 4, "46754061e": 4, "34021775e": 4, "38128232e": 4, "09578541e": 4, "56378723e": 4, "29070841e": 4, "36099930e": 4, "10196943e": 4, "00287900e": 4, "46343326e": 4, "30044727e": 4, "16163127e": 4, "43721746e": 4, "17491091e": 4, "52751313e": 4, "05728725e": 4, "67492444e": 4, "12162175e": 4, "62762087e": 4, "12349084e": 4, "75368562e": 4, "78313626e": 4, "81814841e": 4, "88654257e": 4, "93711794e": 4, "90082072e": 4, "revolutionari": 4, "itself": 4, "hand": 4, "deal": 4, "featur": 4, "everyth": 4, "did": 4, "below": 4, "review": 4, "print": 4, "2023": 4, "08": 4, "59": 4, "173578": 4, "_data_write_config": 4, "compress": 4, "snappi": 4, "use_dictionari": 4, "9a": 4, "fa": 4, "f": 4, "stepr": 4, "b": 4, "z": 4, "z0": 4, "cpamt": 4, "mlhc": 4, "m": 4, "jan": 4, "feb": 4, "mar": 4, "apr": 4, "jun": 4, "jul": 4, "aug": 4, "sep": 4, "oct": 4, "nov": 4, "dec": 4, "ap": 4, "_artifacts_write_config": 4, "w2v_model": 4, "46": 4, "unk_id": 4, "id_to_label_format_arg": 4, "008010": 4, "memori": [4, 5], "mb": 4, "013305": 4, "000863": 4, "003406": 4, "074747": 4, "531624": 4, "003835": 4, "03622": 4, "001360": 4, "008777": 4, "088891": 4, "raw": 5, "integr": 5, "engin": 5, "facilit": 5, "develop": 5, "machin": 5, "emphas": 5, "modular": 5, "compon": 5, "simplifi": 5, "upkeep": 5, "complex": 5, "apach": 5, "columnar": 5, "storag": 5, "enhanc": 5, "speed": 5, "optim": 5, "resourc": 5, "design": 5, "driven": 5, "incorpor": 5, "modul": 5, "seamlessli": 5, "standalon": 5, "context": 5, "comprehens": 5, "growth": 5, "mind": 5, "allow": 5, "straightforward": 5, "therebi": 5, "broaden": 5, "encompass": 5, "unaddress": 5, "demand": 5, "evolv": 5, "instal": 5, "tutori": 5, "normal": 5, "collect": 5, "script": 5, "advanc": 5, "packag": 5, "tokenizer_train": 5, "tokenizer_encod": 5, "librari": 5, "index": 5, "At": 6, "command": 6, "pip": 6, "unix": 6, "platform": 6}, "objects": {"bardi.data": [[1, 0, 0, "-", "data_handlers"]], "bardi.data.data_handlers": [[1, 1, 1, "", "Dataset"], [1, 4, 1, "", "from_duckdb"], [1, 4, 1, "", "from_file"], [1, 4, 1, "", "from_json"], [1, 4, 1, "", "from_pandas"], [1, 4, 1, "", "from_pyarrow"], [1, 4, 1, "", "to_pandas"], [1, 4, 1, "", "to_polars"], [1, 4, 1, "", "write_file"]], "bardi.data.data_handlers.Dataset": [[1, 2, 1, "", "data"], [1, 3, 1, "", "get_parameters"], [1, 2, 1, "", "origin_file_path"], [1, 2, 1, "", "origin_format"], [1, 2, 1, "", "origin_query"], [1, 2, 1, "", "origin_row_count"]], "bardi.nlp_engineering": [[2, 0, 0, "-", "embedding_generator"], [2, 0, 0, "-", "label_processor"], [2, 0, 0, "-", "normalizer"], [2, 0, 0, "-", "pre_tokenizer"], [2, 0, 0, "-", "splitter"], [2, 0, 0, "-", "tokenizer_encoder"], [2, 0, 0, "-", "tokenizer_trainer"], [2, 0, 0, "-", "vocab_encoder"]], "bardi.nlp_engineering.embedding_generator": [[2, 1, 1, "", "CPUEmbeddingGenerator"], [2, 1, 1, "", "EmbeddingGenerator"], [2, 1, 1, "", "EmbeddingGeneratorArtifactsWriteConfig"]], "bardi.nlp_engineering.embedding_generator.CPUEmbeddingGenerator": [[2, 2, 1, "", "checkpoint_path"], [2, 2, 1, "", "cores"], [2, 2, 1, "", "epochs"], [2, 2, 1, "", "fields"], [2, 3, 1, "", "get_parameters"], [2, 2, 1, "", "load_saved_model"], [2, 2, 1, "", "min_alpha"], [2, 2, 1, "", "min_word_count"], [2, 2, 1, "", "negative"], [2, 3, 1, "", "run"], [2, 2, 1, "", "sample"], [2, 2, 1, "", "seed"], [2, 2, 1, "", "vector_size"], [2, 2, 1, "", "vocab_exclude_list"], [2, 2, 1, "", "window"], [2, 3, 1, "", "write_artifacts"]], "bardi.nlp_engineering.embedding_generator.EmbeddingGenerator": [[2, 2, 1, "", "checkpoint_path"], [2, 2, 1, "", "cores"], [2, 2, 1, "", "epochs"], [2, 2, 1, "", "fields"], [2, 2, 1, "", "load_saved_model"], [2, 2, 1, "", "min_alpha"], [2, 2, 1, "", "min_word_count"], [2, 2, 1, "", "negative"], [2, 3, 1, "", "run"], [2, 2, 1, "", "sample"], [2, 2, 1, "", "seed"], [2, 3, 1, "", "set_write_config"], [2, 2, 1, "", "vector_size"], [2, 2, 1, "", "vocab_exclude_list"], [2, 2, 1, "", "window"]], "bardi.nlp_engineering.embedding_generator.EmbeddingGeneratorArtifactsWriteConfig": [[2, 2, 1, "", "embedding_matrix_format"], [2, 2, 1, "", "embedding_matrix_format_args"], [2, 2, 1, "", "vocab_format"], [2, 2, 1, "", "vocab_format_args"]], "bardi.nlp_engineering.label_processor": [[2, 1, 1, "", "CPULabelProcessor"], [2, 1, 1, "", "LabelProcessor"], [2, 1, 1, "", "LabelProcessorArtifactsWriteConfig"]], "bardi.nlp_engineering.label_processor.CPULabelProcessor": [[2, 2, 1, "", "fields"], [2, 3, 1, "", "get_parameters"], [2, 2, 1, "id0", "id_to_label"], [2, 2, 1, "", "mapping"], [2, 2, 1, "", "method"], [2, 2, 1, "", "retain_input_fields"], [2, 3, 1, "", "run"], [2, 3, 1, "", "write_artifacts"]], "bardi.nlp_engineering.label_processor.LabelProcessor": [[2, 2, 1, "", "fields"], [2, 2, 1, "", "id_to_label"], [2, 2, 1, "", "method"], [2, 2, 1, "", "retain_input_fields"], [2, 3, 1, "", "run"], [2, 3, 1, "", "set_write_config"]], "bardi.nlp_engineering.label_processor.LabelProcessorArtifactsWriteConfig": [[2, 2, 1, "", "id_to_label_args"], [2, 2, 1, "", "id_to_label_format"]], "bardi.nlp_engineering.normalizer": [[2, 1, 1, "", "CPUNormalizer"], [2, 1, 1, "", "Normalizer"]], "bardi.nlp_engineering.normalizer.CPUNormalizer": [[2, 2, 1, "", "fields"], [2, 2, 1, "", "lowercase"], [2, 2, 1, "", "regex_set"], [2, 2, 1, "", "retain_input_fields"], [2, 3, 1, "", "run"]], "bardi.nlp_engineering.normalizer.Normalizer": [[2, 2, 1, "", "fields"], [2, 2, 1, "", "lowercase"], [2, 2, 1, "", "regex_set"], [2, 2, 1, "", "retain_input_fields"], [2, 3, 1, "", "run"]], "bardi.nlp_engineering.pre_tokenizer": [[2, 1, 1, "", "CPUPreTokenizer"], [2, 1, 1, "", "PreTokenizer"]], "bardi.nlp_engineering.pre_tokenizer.CPUPreTokenizer": [[2, 2, 1, "", "fields"], [2, 2, 1, "", "retain_input_fields"], [2, 3, 1, "", "run"], [2, 2, 1, "", "split_pattern"]], "bardi.nlp_engineering.pre_tokenizer.PreTokenizer": [[2, 2, 1, "", "fields"], [2, 2, 1, "", "retain_input_fields"], [2, 3, 1, "", "run"], [2, 2, 1, "", "split_pattern"]], "bardi.nlp_engineering.regex_library": [[2, 0, 0, "-", "pathology_report"], [2, 0, 0, "-", "regex_lib"], [2, 0, 0, "-", "regex_set"]], "bardi.nlp_engineering.regex_library.pathology_report": [[2, 1, 1, "", "PathologyReportRegexSet"]], "bardi.nlp_engineering.regex_library.pathology_report.PathologyReportRegexSet": [[2, 2, 1, "", "convert_escape_codes"], [2, 2, 1, "", "fix_pluralization"], [2, 2, 1, "", "handle_angle_brackets"], [2, 2, 1, "", "handle_cassettes_spacing"], [2, 2, 1, "", "handle_dash_digit_spacing"], [2, 2, 1, "", "handle_digits_words_spacing"], [2, 2, 1, "", "handle_dimension_spacing"], [2, 2, 1, "", "handle_leading_digit_punct"], [2, 2, 1, "", "handle_literals_floats_spacing"], [2, 2, 1, "", "handle_math_spacing"], [2, 2, 1, "", "handle_measure_spacing"], [2, 2, 1, "", "handle_whitespaces"], [2, 2, 1, "", "handle_words_with_punct_spacing"], [2, 2, 1, "", "remove_addresses"], [2, 2, 1, "", "remove_cassette_names"], [2, 2, 1, "", "remove_dates"], [2, 2, 1, "", "remove_decimal_seg_numbers"], [2, 2, 1, "", "remove_dimensions"], [2, 2, 1, "", "remove_duration_time"], [2, 2, 1, "", "remove_large_digits_seq"], [2, 2, 1, "", "remove_large_floats_seq"], [2, 2, 1, "", "remove_leading_punct"], [2, 2, 1, "", "remove_letter_num_seq"], [2, 2, 1, "", "remove_multiple_punct"], [2, 2, 1, "", "remove_phone_numbers"], [2, 2, 1, "", "remove_special_punct"], [2, 2, 1, "", "remove_specimen"], [2, 2, 1, "", "remove_times"], [2, 2, 1, "", "remove_trailing_punct"], [2, 2, 1, "", "remove_urls"], [2, 2, 1, "", "replace_percent_sign"], [2, 2, 1, "", "trunc_decimals"]], "bardi.nlp_engineering.regex_library.regex_lib": [[2, 4, 1, "", "get_address_regex"], [2, 4, 1, "", "get_angle_brackets_regex"], [2, 4, 1, "", "get_cassette_name_regex"], [2, 4, 1, "", "get_cassettes_spacing_regex"], [2, 4, 1, "", "get_dash_digits_spacing_regex"], [2, 4, 1, "", "get_dates_regex"], [2, 4, 1, "", "get_decimal_segmented_numbers_regex"], [2, 4, 1, "", "get_digits_words_spacing_regex"], [2, 4, 1, "", "get_dimension_spacing_regex"], [2, 4, 1, "", "get_dimensions_regex"], [2, 4, 1, "", "get_duration_regex"], [2, 4, 1, "", "get_escape_code_regex"], [2, 4, 1, "", "get_fix_pluralization_regex"], [2, 4, 1, "", "get_large_digits_seq_regex"], [2, 4, 1, "", "get_large_float_seq_regex"], [2, 4, 1, "", "get_leading_digit_punctuation_regex"], [2, 4, 1, "", "get_leading_punctuation_regex"], [2, 4, 1, "", "get_letter_num_seq_regex"], [2, 4, 1, "", "get_literals_floats_spacing_regex"], [2, 4, 1, "", "get_math_spacing_regex"], [2, 4, 1, "", "get_measure_spacing_regex"], [2, 4, 1, "", "get_multiple_punct_regex"], [2, 4, 1, "", "get_percent_sign_regex"], [2, 4, 1, "", "get_phone_number_regex"], [2, 4, 1, "", "get_spaces_regex"], [2, 4, 1, "", "get_special_punct_regex"], [2, 4, 1, "", "get_specimen_regex"], [2, 4, 1, "", "get_time_regex"], [2, 4, 1, "", "get_trailing_punctuation_regex"], [2, 4, 1, "", "get_trunc_decimals_regex"], [2, 4, 1, "", "get_urls_regex"], [2, 4, 1, "", "get_whitespace_regex"], [2, 4, 1, "", "get_words_with_punct_spacing_regex"]], "bardi.nlp_engineering.regex_library.regex_set": [[2, 1, 1, "", "RegexSet"], [2, 1, 1, "", "RegexSubPair"]], "bardi.nlp_engineering.regex_library.regex_set.RegexSet": [[2, 3, 1, "", "get_regex_set"], [2, 2, 1, "", "lowercase_substitution"], [2, 2, 1, "", "no_substitution"], [2, 2, 1, "", "regex_set"]], "bardi.nlp_engineering.regex_library.regex_set.RegexSubPair": [[2, 2, 1, "id1", "regex_str"], [2, 2, 1, "id2", "sub_str"]], "bardi.nlp_engineering.splitter": [[2, 1, 1, "", "CPUSplitter"], [2, 1, 1, "", "MapSplit"], [2, 1, 1, "", "NewSplit"], [2, 1, 1, "", "Splitter"]], "bardi.nlp_engineering.splitter.CPUSplitter": [[2, 2, 1, "", "group_cols"], [2, 2, 1, "", "label_cols"], [2, 2, 1, "", "num_splits"], [2, 2, 1, "", "random_seed"], [2, 3, 1, "", "run"], [2, 2, 1, "", "split_mapping"], [2, 2, 1, "", "split_method"], [2, 2, 1, "", "split_proportions"], [2, 2, 1, "", "split_type"], [2, 2, 1, "", "unique_record_cols"]], "bardi.nlp_engineering.splitter.MapSplit": [[2, 2, 1, "", "default_split_value"], [2, 2, 1, "", "split_mapping"], [2, 2, 1, "", "unique_record_cols"]], "bardi.nlp_engineering.splitter.NewSplit": [[2, 2, 1, "", "group_cols"], [2, 2, 1, "", "label_cols"], [2, 2, 1, "", "random_seed"], [2, 2, 1, "", "split_proportions"], [2, 2, 1, "", "unique_record_cols"]], "bardi.nlp_engineering.splitter.Splitter": [[2, 2, 1, "", "group_cols"], [2, 2, 1, "", "label_cols"], [2, 2, 1, "", "num_splits"], [2, 2, 1, "", "random_seed"], [2, 3, 1, "", "run"], [2, 2, 1, "", "split_mapping"], [2, 2, 1, "", "split_method"], [2, 2, 1, "", "split_proportions"], [2, 2, 1, "", "split_type"], [2, 2, 1, "", "unique_record_cols"]], "bardi.nlp_engineering.tokenizer_encoder": [[2, 1, 1, "", "CPUTokenizerEncoder"], [2, 1, 1, "", "TokenizerEncoder"]], "bardi.nlp_engineering.tokenizer_encoder.CPUTokenizerEncoder": [[2, 2, 1, "", "concat_fields"], [2, 2, 1, "", "cores"], [2, 2, 1, "", "field_rename"], [2, 2, 1, "", "fields"], [2, 3, 1, "", "get_parameters"], [2, 2, 1, "", "hf_cache_dir"], [2, 2, 1, "", "model_name"], [2, 2, 1, "", "retain_concat_field"], [2, 2, 1, "", "retain_input_fields"], [2, 2, 1, "", "return_tensors"], [2, 3, 1, "", "run"], [2, 2, 1, "", "tokenizer_model"], [2, 2, 1, "", "tokenizer_params"]], "bardi.nlp_engineering.tokenizer_encoder.TokenizerEncoder": [[2, 2, 1, "", "concat_fields"], [2, 2, 1, "", "cores"], [2, 2, 1, "", "field_rename"], [2, 2, 1, "", "fields"], [2, 2, 1, "", "hf_cache_dir"], [2, 2, 1, "", "model_name"], [2, 2, 1, "", "retain_concat_field"], [2, 2, 1, "", "retain_input_fields"], [2, 2, 1, "", "return_tensors"], [2, 3, 1, "", "run"], [2, 2, 1, "", "tokenizer_model"], [2, 2, 1, "", "tokenizer_params"]], "bardi.nlp_engineering.tokenizer_trainer": [[2, 1, 1, "", "CPUTokenizerTrainer"], [2, 1, 1, "", "TokenizerTrainer"], [2, 1, 1, "", "TokenizerTrainerArtifactsWriteConfig"]], "bardi.nlp_engineering.tokenizer_trainer.CPUTokenizerTrainer": [[2, 2, 1, "", "checkpoint_path"], [2, 2, 1, "", "corpus_gen_batch_size"], [2, 2, 1, "", "field"], [2, 2, 1, "", "from_old_flag"], [2, 3, 1, "", "get_parameters"], [2, 2, 1, "", "hf_cache_dir"], [2, 3, 1, "", "run"], [2, 2, 1, "", "tokenizer_fname"], [2, 2, 1, "", "tokenizer_type"], [2, 2, 1, "", "vocab_size"], [2, 3, 1, "", "write_artifacts"]], "bardi.nlp_engineering.tokenizer_trainer.TokenizerTrainer": [[2, 2, 1, "", "checkpoint_path"], [2, 2, 1, "", "corpus_gen_batch_size"], [2, 2, 1, "", "field"], [2, 2, 1, "", "from_old_flag"], [2, 2, 1, "", "hf_cache_dir"], [2, 3, 1, "", "run"], [2, 3, 1, "", "set_write_config"], [2, 2, 1, "", "tokenizer_fname"], [2, 2, 1, "", "tokenizer_type"], [2, 2, 1, "", "vocab_size"]], "bardi.nlp_engineering.tokenizer_trainer.TokenizerTrainerArtifactsWriteConfig": [[2, 2, 1, "", "vocab_format"], [2, 2, 1, "", "vocab_format_args"]], "bardi.nlp_engineering.vocab_encoder": [[2, 1, 1, "", "CPUVocabEncoder"], [2, 1, 1, "", "VocabEncoder"]], "bardi.nlp_engineering.vocab_encoder.CPUVocabEncoder": [[2, 2, 1, "", "concat_fields"], [2, 2, 1, "", "field_rename"], [2, 2, 1, "", "fields"], [2, 3, 1, "", "get_parameters"], [2, 2, 1, "", "id_to_token"], [2, 2, 1, "", "retain_input_fields"], [2, 3, 1, "", "run"]], "bardi.nlp_engineering.vocab_encoder.VocabEncoder": [[2, 2, 1, "", "concat_fields"], [2, 2, 1, "", "field_rename"], [2, 2, 1, "", "fields"], [2, 2, 1, "", "id_to_token"], [2, 2, 1, "", "retain_input_fields"], [2, 3, 1, "", "run"]], "bardi": [[3, 0, 0, "-", "pipeline"]], "bardi.pipeline": [[3, 1, 1, "", "DataWriteConfig"], [3, 1, 1, "", "Pipeline"], [3, 1, 1, "", "Step"]], "bardi.pipeline.DataWriteConfig": [[3, 2, 1, "", "data_format"], [3, 2, 1, "", "data_format_args"]], "bardi.pipeline.Pipeline": [[3, 3, 1, "", "add_step"], [3, 2, 1, "", "data_filename"], [3, 2, 1, "", "data_write_config"], [3, 2, 1, "", "dataset"], [3, 3, 1, "", "get_parameters"], [3, 3, 1, "", "run_pipeline"], [3, 2, 1, "", "write_outputs"], [3, 2, 1, "", "write_path"]], "bardi.pipeline.Step": [[3, 3, 1, "", "get_parameters"], [3, 3, 1, "", "run"], [3, 3, 1, "", "set_write_config"], [3, 3, 1, "", "write_artifacts"], [3, 3, 1, "", "write_data"]]}, "objtypes": {"0": "py:module", "1": "py:class", "2": "py:attribute", "3": "py:method", "4": "py:function"}, "objnames": {"0": ["py", "module", "Python module"], "1": ["py", "class", "Python class"], "2": ["py", "attribute", "Python attribute"], "3": ["py", "method", "Python method"], "4": ["py", "function", "Python function"]}, "titleterms": {"advanc": 0, "tutori": [0, 4], "normal": [0, 2, 4], "custom": 0, "creat": 0, "pipelin": [0, 3, 4], "step": [0, 2], "bardi": [1, 2, 3, 4, 5], "data": [1, 4], "packag": [1, 2], "data_handl": 1, "modul": [1, 2, 3], "nlp_engin": 2, "pre_token": 2, "tokenizer_train": 2, "embedding_gener": 2, "vocab_encod": 2, "tokenizer_encod": 2, "label_processor": 2, "splitter": 2, "regex": 2, "regex_set": 2, "provid": 2, "set": [2, 4], "librari": 2, "basic": 4, "prepar": 4, "sampl": 4, "regist": 4, "dataset": 4, "initi": 4, "pre": 4, "process": 4, "ad": 4, "our": 4, "pretoken": 4, "an": 4, "embeddinggener": 4, "vocabencod": 4, "labelprocessor": 4, "run": 4, "result": 4, "collect": 4, "metadata": 4, "full": 4, "script": 4, "welcom": 5, "": 5, "document": 5, "content": 5, "indic": 5, "tabl": 5, "instal": 6}, "envversion": {"sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 60}, "alltitles": {"Advanced Tutorials": [[0, "advanced-tutorials"]], "Normalizer Customizations": [[0, "normalizer-customizations"]], "Creating a Custom Pipeline Step": [[0, "creating-a-custom-pipeline-step"]], "bardi.data package": [[1, "bardi-data-package"]], "bardi.data.data_handlers module": [[1, "module-bardi.data.data_handlers"]], "bardi.nlp_engineering package": [[2, "bardi-nlp-engineering-package"]], "Steps": [[2, "steps"]], "bardi normalizer module": [[2, "module-bardi.nlp_engineering.normalizer"]], "bardi pre_tokenizer module": [[2, "module-bardi.nlp_engineering.pre_tokenizer"]], "bardi tokenizer_trainer module": [[2, "module-bardi.nlp_engineering.tokenizer_trainer"]], "bardi embedding_generator module": [[2, "module-bardi.nlp_engineering.embedding_generator"]], "bardi vocab_encoder module": [[2, "module-bardi.nlp_engineering.vocab_encoder"]], "bardi tokenizer_encoder module": [[2, "module-bardi.nlp_engineering.tokenizer_encoder"]], "bardi label_processor module": [[2, "module-bardi.nlp_engineering.label_processor"]], "bardi splitter module": [[2, "module-bardi.nlp_engineering.splitter"]], "RegEx Package": [[2, "regex-package"]], "bardi regex_set module": [[2, "module-bardi.nlp_engineering.regex_library.regex_set"]], "bardi provided regex sets": [[2, "module-bardi.nlp_engineering.regex_library.pathology_report"]], "bardi provided regex library": [[2, "module-bardi.nlp_engineering.regex_library.regex_lib"]], "bardi.pipeline": [[3, "bardi-pipeline"]], "bardi.pipeline module": [[3, "module-bardi.pipeline"]], "Basic Tutorial": [[4, "basic-tutorial"]], "Preparing a Sample Set of Data": [[4, "preparing-a-sample-set-of-data"]], "Register the Sample Data as a Bardi Dataset": [[4, "register-the-sample-data-as-a-bardi-dataset"]], "Initialize a Pre-Processing Pipeline": [[4, "initialize-a-pre-processing-pipeline"]], "Adding a Normalizer to our Pipeline": [[4, "adding-a-normalizer-to-our-pipeline"]], "Adding a PreTokenizer": [[4, "adding-a-pretokenizer"]], "Adding an EmbeddingGenerator": [[4, "adding-an-embeddinggenerator"]], "Adding a VocabEncoder": [[4, "adding-a-vocabencoder"]], "Adding a LabelProcessor": [[4, "adding-a-labelprocessor"]], "Running the Pipeline": [[4, "running-the-pipeline"]], "Results": [[4, "results"]], "Collecting Metadata": [[4, "collecting-metadata"]], "Full Tutorial Script": [[4, "full-tutorial-script"]], "Welcome to bardi\u2019s documentation!": [[5, "welcome-to-bardi-s-documentation"]], "Contents:": [[5, null]], "Indices and tables": [[5, "indices-and-tables"]], "Installation": [[6, "installation"]]}, "indexentries": {"dataset (class in bardi.data.data_handlers)": [[1, "bardi.data.data_handlers.Dataset"]], "bardi.data.data_handlers": [[1, "module-bardi.data.data_handlers"]], "data (bardi.data.data_handlers.dataset attribute)": [[1, "bardi.data.data_handlers.Dataset.data"]], "from_duckdb() (in module bardi.data.data_handlers)": [[1, "bardi.data.data_handlers.from_duckdb"]], "from_file() (in module bardi.data.data_handlers)": [[1, "bardi.data.data_handlers.from_file"]], "from_json() (in module bardi.data.data_handlers)": [[1, "bardi.data.data_handlers.from_json"]], "from_pandas() (in module bardi.data.data_handlers)": [[1, "bardi.data.data_handlers.from_pandas"]], "from_pyarrow() (in module bardi.data.data_handlers)": [[1, "bardi.data.data_handlers.from_pyarrow"]], "get_parameters() (bardi.data.data_handlers.dataset method)": [[1, "bardi.data.data_handlers.Dataset.get_parameters"]], "module": [[1, "module-bardi.data.data_handlers"], [2, "module-bardi.nlp_engineering.embedding_generator"], [2, "module-bardi.nlp_engineering.label_processor"], [2, "module-bardi.nlp_engineering.normalizer"], [2, "module-bardi.nlp_engineering.pre_tokenizer"], [2, "module-bardi.nlp_engineering.regex_library.pathology_report"], [2, "module-bardi.nlp_engineering.regex_library.regex_lib"], [2, "module-bardi.nlp_engineering.regex_library.regex_set"], [2, "module-bardi.nlp_engineering.splitter"], [2, "module-bardi.nlp_engineering.tokenizer_encoder"], [2, "module-bardi.nlp_engineering.tokenizer_trainer"], [2, "module-bardi.nlp_engineering.vocab_encoder"], [3, "module-bardi.pipeline"]], "origin_file_path (bardi.data.data_handlers.dataset attribute)": [[1, "bardi.data.data_handlers.Dataset.origin_file_path"]], "origin_format (bardi.data.data_handlers.dataset attribute)": [[1, "bardi.data.data_handlers.Dataset.origin_format"]], "origin_query (bardi.data.data_handlers.dataset attribute)": [[1, "bardi.data.data_handlers.Dataset.origin_query"]], "origin_row_count (bardi.data.data_handlers.dataset attribute)": [[1, "bardi.data.data_handlers.Dataset.origin_row_count"]], "to_pandas() (in module bardi.data.data_handlers)": [[1, "bardi.data.data_handlers.to_pandas"]], "to_polars() (in module bardi.data.data_handlers)": [[1, "bardi.data.data_handlers.to_polars"]], "write_file() (in module bardi.data.data_handlers)": [[1, "bardi.data.data_handlers.write_file"]], "cpuembeddinggenerator (class in bardi.nlp_engineering.embedding_generator)": [[2, "bardi.nlp_engineering.embedding_generator.CPUEmbeddingGenerator"]], "cpulabelprocessor (class in bardi.nlp_engineering.label_processor)": [[2, "bardi.nlp_engineering.label_processor.CPULabelProcessor"]], "cpunormalizer (class in bardi.nlp_engineering.normalizer)": [[2, "bardi.nlp_engineering.normalizer.CPUNormalizer"]], "cpupretokenizer (class in bardi.nlp_engineering.pre_tokenizer)": [[2, "bardi.nlp_engineering.pre_tokenizer.CPUPreTokenizer"]], "cpusplitter (class in bardi.nlp_engineering.splitter)": [[2, "bardi.nlp_engineering.splitter.CPUSplitter"]], "cputokenizerencoder (class in bardi.nlp_engineering.tokenizer_encoder)": [[2, "bardi.nlp_engineering.tokenizer_encoder.CPUTokenizerEncoder"]], "cputokenizertrainer (class in bardi.nlp_engineering.tokenizer_trainer)": [[2, "bardi.nlp_engineering.tokenizer_trainer.CPUTokenizerTrainer"]], "cpuvocabencoder (class in bardi.nlp_engineering.vocab_encoder)": [[2, "bardi.nlp_engineering.vocab_encoder.CPUVocabEncoder"]], "embeddinggenerator (class in bardi.nlp_engineering.embedding_generator)": [[2, "bardi.nlp_engineering.embedding_generator.EmbeddingGenerator"]], "embeddinggeneratorartifactswriteconfig (class in bardi.nlp_engineering.embedding_generator)": [[2, "bardi.nlp_engineering.embedding_generator.EmbeddingGeneratorArtifactsWriteConfig"]], "labelprocessor (class in bardi.nlp_engineering.label_processor)": [[2, "bardi.nlp_engineering.label_processor.LabelProcessor"]], "labelprocessorartifactswriteconfig (class in bardi.nlp_engineering.label_processor)": [[2, "bardi.nlp_engineering.label_processor.LabelProcessorArtifactsWriteConfig"]], "mapsplit (class in bardi.nlp_engineering.splitter)": [[2, "bardi.nlp_engineering.splitter.MapSplit"]], "newsplit (class in bardi.nlp_engineering.splitter)": [[2, "bardi.nlp_engineering.splitter.NewSplit"]], "normalizer (class in bardi.nlp_engineering.normalizer)": [[2, "bardi.nlp_engineering.normalizer.Normalizer"]], "pathologyreportregexset (class in bardi.nlp_engineering.regex_library.pathology_report)": [[2, "bardi.nlp_engineering.regex_library.pathology_report.PathologyReportRegexSet"]], "pretokenizer (class in bardi.nlp_engineering.pre_tokenizer)": [[2, "bardi.nlp_engineering.pre_tokenizer.PreTokenizer"]], "regexset (class in bardi.nlp_engineering.regex_library.regex_set)": [[2, "bardi.nlp_engineering.regex_library.regex_set.RegexSet"]], "regexsubpair (class in bardi.nlp_engineering.regex_library.regex_set)": [[2, "bardi.nlp_engineering.regex_library.regex_set.RegexSubPair"]], "splitter (class in bardi.nlp_engineering.splitter)": [[2, "bardi.nlp_engineering.splitter.Splitter"]], "tokenizerencoder (class in bardi.nlp_engineering.tokenizer_encoder)": [[2, "bardi.nlp_engineering.tokenizer_encoder.TokenizerEncoder"]], "tokenizertrainer (class in bardi.nlp_engineering.tokenizer_trainer)": [[2, "bardi.nlp_engineering.tokenizer_trainer.TokenizerTrainer"]], "tokenizertrainerartifactswriteconfig (class in bardi.nlp_engineering.tokenizer_trainer)": [[2, "bardi.nlp_engineering.tokenizer_trainer.TokenizerTrainerArtifactsWriteConfig"]], "vocabencoder (class in bardi.nlp_engineering.vocab_encoder)": [[2, "bardi.nlp_engineering.vocab_encoder.VocabEncoder"]], "bardi.nlp_engineering.embedding_generator": [[2, "module-bardi.nlp_engineering.embedding_generator"]], "bardi.nlp_engineering.label_processor": [[2, "module-bardi.nlp_engineering.label_processor"]], "bardi.nlp_engineering.normalizer": [[2, "module-bardi.nlp_engineering.normalizer"]], "bardi.nlp_engineering.pre_tokenizer": [[2, "module-bardi.nlp_engineering.pre_tokenizer"]], "bardi.nlp_engineering.regex_library.pathology_report": [[2, "module-bardi.nlp_engineering.regex_library.pathology_report"]], "bardi.nlp_engineering.regex_library.regex_lib": [[2, "module-bardi.nlp_engineering.regex_library.regex_lib"]], "bardi.nlp_engineering.regex_library.regex_set": [[2, "module-bardi.nlp_engineering.regex_library.regex_set"]], "bardi.nlp_engineering.splitter": [[2, "module-bardi.nlp_engineering.splitter"]], "bardi.nlp_engineering.tokenizer_encoder": [[2, "module-bardi.nlp_engineering.tokenizer_encoder"]], "bardi.nlp_engineering.tokenizer_trainer": [[2, "module-bardi.nlp_engineering.tokenizer_trainer"]], "bardi.nlp_engineering.vocab_encoder": [[2, "module-bardi.nlp_engineering.vocab_encoder"]], "checkpoint_path (bardi.nlp_engineering.embedding_generator.cpuembeddinggenerator attribute)": [[2, "bardi.nlp_engineering.embedding_generator.CPUEmbeddingGenerator.checkpoint_path"]], "checkpoint_path (bardi.nlp_engineering.embedding_generator.embeddinggenerator attribute)": [[2, "bardi.nlp_engineering.embedding_generator.EmbeddingGenerator.checkpoint_path"]], "checkpoint_path (bardi.nlp_engineering.tokenizer_trainer.cputokenizertrainer attribute)": [[2, "bardi.nlp_engineering.tokenizer_trainer.CPUTokenizerTrainer.checkpoint_path"]], "checkpoint_path (bardi.nlp_engineering.tokenizer_trainer.tokenizertrainer attribute)": [[2, "bardi.nlp_engineering.tokenizer_trainer.TokenizerTrainer.checkpoint_path"]], "concat_fields (bardi.nlp_engineering.tokenizer_encoder.cputokenizerencoder attribute)": [[2, "bardi.nlp_engineering.tokenizer_encoder.CPUTokenizerEncoder.concat_fields"]], "concat_fields (bardi.nlp_engineering.tokenizer_encoder.tokenizerencoder attribute)": [[2, "bardi.nlp_engineering.tokenizer_encoder.TokenizerEncoder.concat_fields"]], "concat_fields (bardi.nlp_engineering.vocab_encoder.cpuvocabencoder attribute)": [[2, "bardi.nlp_engineering.vocab_encoder.CPUVocabEncoder.concat_fields"]], "concat_fields (bardi.nlp_engineering.vocab_encoder.vocabencoder attribute)": [[2, "bardi.nlp_engineering.vocab_encoder.VocabEncoder.concat_fields"]], "convert_escape_codes (bardi.nlp_engineering.regex_library.pathology_report.pathologyreportregexset attribute)": [[2, "bardi.nlp_engineering.regex_library.pathology_report.PathologyReportRegexSet.convert_escape_codes"]], "cores (bardi.nlp_engineering.embedding_generator.cpuembeddinggenerator attribute)": [[2, "bardi.nlp_engineering.embedding_generator.CPUEmbeddingGenerator.cores"]], "cores (bardi.nlp_engineering.embedding_generator.embeddinggenerator attribute)": [[2, "bardi.nlp_engineering.embedding_generator.EmbeddingGenerator.cores"]], "cores (bardi.nlp_engineering.tokenizer_encoder.cputokenizerencoder attribute)": [[2, "bardi.nlp_engineering.tokenizer_encoder.CPUTokenizerEncoder.cores"]], "cores (bardi.nlp_engineering.tokenizer_encoder.tokenizerencoder attribute)": [[2, "bardi.nlp_engineering.tokenizer_encoder.TokenizerEncoder.cores"]], "corpus_gen_batch_size (bardi.nlp_engineering.tokenizer_trainer.cputokenizertrainer attribute)": [[2, "bardi.nlp_engineering.tokenizer_trainer.CPUTokenizerTrainer.corpus_gen_batch_size"]], "corpus_gen_batch_size (bardi.nlp_engineering.tokenizer_trainer.tokenizertrainer attribute)": [[2, "bardi.nlp_engineering.tokenizer_trainer.TokenizerTrainer.corpus_gen_batch_size"]], "default_split_value (bardi.nlp_engineering.splitter.mapsplit attribute)": [[2, "bardi.nlp_engineering.splitter.MapSplit.default_split_value"]], "embedding_matrix_format (bardi.nlp_engineering.embedding_generator.embeddinggeneratorartifactswriteconfig attribute)": [[2, "bardi.nlp_engineering.embedding_generator.EmbeddingGeneratorArtifactsWriteConfig.embedding_matrix_format"]], "embedding_matrix_format_args (bardi.nlp_engineering.embedding_generator.embeddinggeneratorartifactswriteconfig attribute)": [[2, "bardi.nlp_engineering.embedding_generator.EmbeddingGeneratorArtifactsWriteConfig.embedding_matrix_format_args"]], "epochs (bardi.nlp_engineering.embedding_generator.cpuembeddinggenerator attribute)": [[2, "bardi.nlp_engineering.embedding_generator.CPUEmbeddingGenerator.epochs"]], "epochs (bardi.nlp_engineering.embedding_generator.embeddinggenerator attribute)": [[2, "bardi.nlp_engineering.embedding_generator.EmbeddingGenerator.epochs"]], "field (bardi.nlp_engineering.tokenizer_trainer.cputokenizertrainer attribute)": [[2, "bardi.nlp_engineering.tokenizer_trainer.CPUTokenizerTrainer.field"]], "field (bardi.nlp_engineering.tokenizer_trainer.tokenizertrainer attribute)": [[2, "bardi.nlp_engineering.tokenizer_trainer.TokenizerTrainer.field"]], "field_rename (bardi.nlp_engineering.tokenizer_encoder.cputokenizerencoder attribute)": [[2, "bardi.nlp_engineering.tokenizer_encoder.CPUTokenizerEncoder.field_rename"]], "field_rename (bardi.nlp_engineering.tokenizer_encoder.tokenizerencoder attribute)": [[2, "bardi.nlp_engineering.tokenizer_encoder.TokenizerEncoder.field_rename"]], "field_rename (bardi.nlp_engineering.vocab_encoder.cpuvocabencoder attribute)": [[2, "bardi.nlp_engineering.vocab_encoder.CPUVocabEncoder.field_rename"]], "field_rename (bardi.nlp_engineering.vocab_encoder.vocabencoder attribute)": [[2, "bardi.nlp_engineering.vocab_encoder.VocabEncoder.field_rename"]], "fields (bardi.nlp_engineering.embedding_generator.cpuembeddinggenerator attribute)": [[2, "bardi.nlp_engineering.embedding_generator.CPUEmbeddingGenerator.fields"]], "fields (bardi.nlp_engineering.embedding_generator.embeddinggenerator attribute)": [[2, "bardi.nlp_engineering.embedding_generator.EmbeddingGenerator.fields"]], "fields (bardi.nlp_engineering.label_processor.cpulabelprocessor attribute)": [[2, "bardi.nlp_engineering.label_processor.CPULabelProcessor.fields"]], "fields (bardi.nlp_engineering.label_processor.labelprocessor attribute)": [[2, "bardi.nlp_engineering.label_processor.LabelProcessor.fields"]], "fields (bardi.nlp_engineering.normalizer.cpunormalizer attribute)": [[2, "bardi.nlp_engineering.normalizer.CPUNormalizer.fields"]], "fields (bardi.nlp_engineering.normalizer.normalizer attribute)": [[2, "bardi.nlp_engineering.normalizer.Normalizer.fields"]], "fields (bardi.nlp_engineering.pre_tokenizer.cpupretokenizer attribute)": [[2, "bardi.nlp_engineering.pre_tokenizer.CPUPreTokenizer.fields"]], "fields (bardi.nlp_engineering.pre_tokenizer.pretokenizer attribute)": [[2, "bardi.nlp_engineering.pre_tokenizer.PreTokenizer.fields"]], "fields (bardi.nlp_engineering.tokenizer_encoder.cputokenizerencoder attribute)": [[2, "bardi.nlp_engineering.tokenizer_encoder.CPUTokenizerEncoder.fields"]], "fields (bardi.nlp_engineering.tokenizer_encoder.tokenizerencoder attribute)": [[2, "bardi.nlp_engineering.tokenizer_encoder.TokenizerEncoder.fields"]], "fields (bardi.nlp_engineering.vocab_encoder.cpuvocabencoder attribute)": [[2, "bardi.nlp_engineering.vocab_encoder.CPUVocabEncoder.fields"]], "fields (bardi.nlp_engineering.vocab_encoder.vocabencoder attribute)": [[2, "bardi.nlp_engineering.vocab_encoder.VocabEncoder.fields"]], "fix_pluralization (bardi.nlp_engineering.regex_library.pathology_report.pathologyreportregexset attribute)": [[2, "bardi.nlp_engineering.regex_library.pathology_report.PathologyReportRegexSet.fix_pluralization"]], "from_old_flag (bardi.nlp_engineering.tokenizer_trainer.cputokenizertrainer attribute)": [[2, "bardi.nlp_engineering.tokenizer_trainer.CPUTokenizerTrainer.from_old_flag"]], "from_old_flag (bardi.nlp_engineering.tokenizer_trainer.tokenizertrainer attribute)": [[2, "bardi.nlp_engineering.tokenizer_trainer.TokenizerTrainer.from_old_flag"]], "get_address_regex() (in module bardi.nlp_engineering.regex_library.regex_lib)": [[2, "bardi.nlp_engineering.regex_library.regex_lib.get_address_regex"]], "get_angle_brackets_regex() (in module bardi.nlp_engineering.regex_library.regex_lib)": [[2, "bardi.nlp_engineering.regex_library.regex_lib.get_angle_brackets_regex"]], "get_cassette_name_regex() (in module bardi.nlp_engineering.regex_library.regex_lib)": [[2, "bardi.nlp_engineering.regex_library.regex_lib.get_cassette_name_regex"]], "get_cassettes_spacing_regex() (in module bardi.nlp_engineering.regex_library.regex_lib)": [[2, "bardi.nlp_engineering.regex_library.regex_lib.get_cassettes_spacing_regex"]], "get_dash_digits_spacing_regex() (in module bardi.nlp_engineering.regex_library.regex_lib)": [[2, "bardi.nlp_engineering.regex_library.regex_lib.get_dash_digits_spacing_regex"]], "get_dates_regex() (in module bardi.nlp_engineering.regex_library.regex_lib)": [[2, "bardi.nlp_engineering.regex_library.regex_lib.get_dates_regex"]], "get_decimal_segmented_numbers_regex() (in module bardi.nlp_engineering.regex_library.regex_lib)": [[2, "bardi.nlp_engineering.regex_library.regex_lib.get_decimal_segmented_numbers_regex"]], "get_digits_words_spacing_regex() (in module bardi.nlp_engineering.regex_library.regex_lib)": [[2, "bardi.nlp_engineering.regex_library.regex_lib.get_digits_words_spacing_regex"]], "get_dimension_spacing_regex() (in module bardi.nlp_engineering.regex_library.regex_lib)": [[2, "bardi.nlp_engineering.regex_library.regex_lib.get_dimension_spacing_regex"]], "get_dimensions_regex() (in module bardi.nlp_engineering.regex_library.regex_lib)": [[2, "bardi.nlp_engineering.regex_library.regex_lib.get_dimensions_regex"]], "get_duration_regex() (in module bardi.nlp_engineering.regex_library.regex_lib)": [[2, "bardi.nlp_engineering.regex_library.regex_lib.get_duration_regex"]], "get_escape_code_regex() (in module bardi.nlp_engineering.regex_library.regex_lib)": [[2, "bardi.nlp_engineering.regex_library.regex_lib.get_escape_code_regex"]], "get_fix_pluralization_regex() (in module bardi.nlp_engineering.regex_library.regex_lib)": [[2, "bardi.nlp_engineering.regex_library.regex_lib.get_fix_pluralization_regex"]], "get_large_digits_seq_regex() (in module bardi.nlp_engineering.regex_library.regex_lib)": [[2, "bardi.nlp_engineering.regex_library.regex_lib.get_large_digits_seq_regex"]], "get_large_float_seq_regex() (in module bardi.nlp_engineering.regex_library.regex_lib)": [[2, "bardi.nlp_engineering.regex_library.regex_lib.get_large_float_seq_regex"]], "get_leading_digit_punctuation_regex() (in module bardi.nlp_engineering.regex_library.regex_lib)": [[2, "bardi.nlp_engineering.regex_library.regex_lib.get_leading_digit_punctuation_regex"]], "get_leading_punctuation_regex() (in module bardi.nlp_engineering.regex_library.regex_lib)": [[2, "bardi.nlp_engineering.regex_library.regex_lib.get_leading_punctuation_regex"]], "get_letter_num_seq_regex() (in module bardi.nlp_engineering.regex_library.regex_lib)": [[2, "bardi.nlp_engineering.regex_library.regex_lib.get_letter_num_seq_regex"]], "get_literals_floats_spacing_regex() (in module bardi.nlp_engineering.regex_library.regex_lib)": [[2, "bardi.nlp_engineering.regex_library.regex_lib.get_literals_floats_spacing_regex"]], "get_math_spacing_regex() (in module bardi.nlp_engineering.regex_library.regex_lib)": [[2, "bardi.nlp_engineering.regex_library.regex_lib.get_math_spacing_regex"]], "get_measure_spacing_regex() (in module bardi.nlp_engineering.regex_library.regex_lib)": [[2, "bardi.nlp_engineering.regex_library.regex_lib.get_measure_spacing_regex"]], "get_multiple_punct_regex() (in module bardi.nlp_engineering.regex_library.regex_lib)": [[2, "bardi.nlp_engineering.regex_library.regex_lib.get_multiple_punct_regex"]], "get_parameters() (bardi.nlp_engineering.embedding_generator.cpuembeddinggenerator method)": [[2, "bardi.nlp_engineering.embedding_generator.CPUEmbeddingGenerator.get_parameters"]], "get_parameters() (bardi.nlp_engineering.label_processor.cpulabelprocessor method)": [[2, "bardi.nlp_engineering.label_processor.CPULabelProcessor.get_parameters"]], "get_parameters() (bardi.nlp_engineering.tokenizer_encoder.cputokenizerencoder method)": [[2, "bardi.nlp_engineering.tokenizer_encoder.CPUTokenizerEncoder.get_parameters"]], "get_parameters() (bardi.nlp_engineering.tokenizer_trainer.cputokenizertrainer method)": [[2, "bardi.nlp_engineering.tokenizer_trainer.CPUTokenizerTrainer.get_parameters"]], "get_parameters() (bardi.nlp_engineering.vocab_encoder.cpuvocabencoder method)": [[2, "bardi.nlp_engineering.vocab_encoder.CPUVocabEncoder.get_parameters"]], "get_percent_sign_regex() (in module bardi.nlp_engineering.regex_library.regex_lib)": [[2, "bardi.nlp_engineering.regex_library.regex_lib.get_percent_sign_regex"]], "get_phone_number_regex() (in module bardi.nlp_engineering.regex_library.regex_lib)": [[2, "bardi.nlp_engineering.regex_library.regex_lib.get_phone_number_regex"]], "get_regex_set() (bardi.nlp_engineering.regex_library.regex_set.regexset method)": [[2, "bardi.nlp_engineering.regex_library.regex_set.RegexSet.get_regex_set"]], "get_spaces_regex() (in module bardi.nlp_engineering.regex_library.regex_lib)": [[2, "bardi.nlp_engineering.regex_library.regex_lib.get_spaces_regex"]], "get_special_punct_regex() (in module bardi.nlp_engineering.regex_library.regex_lib)": [[2, "bardi.nlp_engineering.regex_library.regex_lib.get_special_punct_regex"]], "get_specimen_regex() (in module bardi.nlp_engineering.regex_library.regex_lib)": [[2, "bardi.nlp_engineering.regex_library.regex_lib.get_specimen_regex"]], "get_time_regex() (in module bardi.nlp_engineering.regex_library.regex_lib)": [[2, "bardi.nlp_engineering.regex_library.regex_lib.get_time_regex"]], "get_trailing_punctuation_regex() (in module bardi.nlp_engineering.regex_library.regex_lib)": [[2, "bardi.nlp_engineering.regex_library.regex_lib.get_trailing_punctuation_regex"]], "get_trunc_decimals_regex() (in module bardi.nlp_engineering.regex_library.regex_lib)": [[2, "bardi.nlp_engineering.regex_library.regex_lib.get_trunc_decimals_regex"]], "get_urls_regex() (in module bardi.nlp_engineering.regex_library.regex_lib)": [[2, "bardi.nlp_engineering.regex_library.regex_lib.get_urls_regex"]], "get_whitespace_regex() (in module bardi.nlp_engineering.regex_library.regex_lib)": [[2, "bardi.nlp_engineering.regex_library.regex_lib.get_whitespace_regex"]], "get_words_with_punct_spacing_regex() (in module bardi.nlp_engineering.regex_library.regex_lib)": [[2, "bardi.nlp_engineering.regex_library.regex_lib.get_words_with_punct_spacing_regex"]], "group_cols (bardi.nlp_engineering.splitter.cpusplitter attribute)": [[2, "bardi.nlp_engineering.splitter.CPUSplitter.group_cols"]], "group_cols (bardi.nlp_engineering.splitter.newsplit attribute)": [[2, "bardi.nlp_engineering.splitter.NewSplit.group_cols"]], "group_cols (bardi.nlp_engineering.splitter.splitter attribute)": [[2, "bardi.nlp_engineering.splitter.Splitter.group_cols"]], "handle_angle_brackets (bardi.nlp_engineering.regex_library.pathology_report.pathologyreportregexset attribute)": [[2, "bardi.nlp_engineering.regex_library.pathology_report.PathologyReportRegexSet.handle_angle_brackets"]], "handle_cassettes_spacing (bardi.nlp_engineering.regex_library.pathology_report.pathologyreportregexset attribute)": [[2, "bardi.nlp_engineering.regex_library.pathology_report.PathologyReportRegexSet.handle_cassettes_spacing"]], "handle_dash_digit_spacing (bardi.nlp_engineering.regex_library.pathology_report.pathologyreportregexset attribute)": [[2, "bardi.nlp_engineering.regex_library.pathology_report.PathologyReportRegexSet.handle_dash_digit_spacing"]], "handle_digits_words_spacing (bardi.nlp_engineering.regex_library.pathology_report.pathologyreportregexset attribute)": [[2, "bardi.nlp_engineering.regex_library.pathology_report.PathologyReportRegexSet.handle_digits_words_spacing"]], "handle_dimension_spacing (bardi.nlp_engineering.regex_library.pathology_report.pathologyreportregexset attribute)": [[2, "bardi.nlp_engineering.regex_library.pathology_report.PathologyReportRegexSet.handle_dimension_spacing"]], "handle_leading_digit_punct (bardi.nlp_engineering.regex_library.pathology_report.pathologyreportregexset attribute)": [[2, "bardi.nlp_engineering.regex_library.pathology_report.PathologyReportRegexSet.handle_leading_digit_punct"]], "handle_literals_floats_spacing (bardi.nlp_engineering.regex_library.pathology_report.pathologyreportregexset attribute)": [[2, "bardi.nlp_engineering.regex_library.pathology_report.PathologyReportRegexSet.handle_literals_floats_spacing"]], "handle_math_spacing (bardi.nlp_engineering.regex_library.pathology_report.pathologyreportregexset attribute)": [[2, "bardi.nlp_engineering.regex_library.pathology_report.PathologyReportRegexSet.handle_math_spacing"]], "handle_measure_spacing (bardi.nlp_engineering.regex_library.pathology_report.pathologyreportregexset attribute)": [[2, "bardi.nlp_engineering.regex_library.pathology_report.PathologyReportRegexSet.handle_measure_spacing"]], "handle_whitespaces (bardi.nlp_engineering.regex_library.pathology_report.pathologyreportregexset attribute)": [[2, "bardi.nlp_engineering.regex_library.pathology_report.PathologyReportRegexSet.handle_whitespaces"]], "handle_words_with_punct_spacing (bardi.nlp_engineering.regex_library.pathology_report.pathologyreportregexset attribute)": [[2, "bardi.nlp_engineering.regex_library.pathology_report.PathologyReportRegexSet.handle_words_with_punct_spacing"]], "hf_cache_dir (bardi.nlp_engineering.tokenizer_encoder.cputokenizerencoder attribute)": [[2, "bardi.nlp_engineering.tokenizer_encoder.CPUTokenizerEncoder.hf_cache_dir"]], "hf_cache_dir (bardi.nlp_engineering.tokenizer_encoder.tokenizerencoder attribute)": [[2, "bardi.nlp_engineering.tokenizer_encoder.TokenizerEncoder.hf_cache_dir"]], "hf_cache_dir (bardi.nlp_engineering.tokenizer_trainer.cputokenizertrainer attribute)": [[2, "bardi.nlp_engineering.tokenizer_trainer.CPUTokenizerTrainer.hf_cache_dir"]], "hf_cache_dir (bardi.nlp_engineering.tokenizer_trainer.tokenizertrainer attribute)": [[2, "bardi.nlp_engineering.tokenizer_trainer.TokenizerTrainer.hf_cache_dir"]], "id_to_label (bardi.nlp_engineering.label_processor.cpulabelprocessor attribute)": [[2, "bardi.nlp_engineering.label_processor.CPULabelProcessor.id_to_label"], [2, "id0"]], "id_to_label (bardi.nlp_engineering.label_processor.labelprocessor attribute)": [[2, "bardi.nlp_engineering.label_processor.LabelProcessor.id_to_label"]], "id_to_label_args (bardi.nlp_engineering.label_processor.labelprocessorartifactswriteconfig attribute)": [[2, "bardi.nlp_engineering.label_processor.LabelProcessorArtifactsWriteConfig.id_to_label_args"]], "id_to_label_format (bardi.nlp_engineering.label_processor.labelprocessorartifactswriteconfig attribute)": [[2, "bardi.nlp_engineering.label_processor.LabelProcessorArtifactsWriteConfig.id_to_label_format"]], "id_to_token (bardi.nlp_engineering.vocab_encoder.cpuvocabencoder attribute)": [[2, "bardi.nlp_engineering.vocab_encoder.CPUVocabEncoder.id_to_token"]], "id_to_token (bardi.nlp_engineering.vocab_encoder.vocabencoder attribute)": [[2, "bardi.nlp_engineering.vocab_encoder.VocabEncoder.id_to_token"]], "label_cols (bardi.nlp_engineering.splitter.cpusplitter attribute)": [[2, "bardi.nlp_engineering.splitter.CPUSplitter.label_cols"]], "label_cols (bardi.nlp_engineering.splitter.newsplit attribute)": [[2, "bardi.nlp_engineering.splitter.NewSplit.label_cols"]], "label_cols (bardi.nlp_engineering.splitter.splitter attribute)": [[2, "bardi.nlp_engineering.splitter.Splitter.label_cols"]], "load_saved_model (bardi.nlp_engineering.embedding_generator.cpuembeddinggenerator attribute)": [[2, "bardi.nlp_engineering.embedding_generator.CPUEmbeddingGenerator.load_saved_model"]], "load_saved_model (bardi.nlp_engineering.embedding_generator.embeddinggenerator attribute)": [[2, "bardi.nlp_engineering.embedding_generator.EmbeddingGenerator.load_saved_model"]], "lowercase (bardi.nlp_engineering.normalizer.cpunormalizer attribute)": [[2, "bardi.nlp_engineering.normalizer.CPUNormalizer.lowercase"]], "lowercase (bardi.nlp_engineering.normalizer.normalizer attribute)": [[2, "bardi.nlp_engineering.normalizer.Normalizer.lowercase"]], "lowercase_substitution (bardi.nlp_engineering.regex_library.regex_set.regexset attribute)": [[2, "bardi.nlp_engineering.regex_library.regex_set.RegexSet.lowercase_substitution"]], "mapping (bardi.nlp_engineering.label_processor.cpulabelprocessor attribute)": [[2, "bardi.nlp_engineering.label_processor.CPULabelProcessor.mapping"]], "method (bardi.nlp_engineering.label_processor.cpulabelprocessor attribute)": [[2, "bardi.nlp_engineering.label_processor.CPULabelProcessor.method"]], "method (bardi.nlp_engineering.label_processor.labelprocessor attribute)": [[2, "bardi.nlp_engineering.label_processor.LabelProcessor.method"]], "min_alpha (bardi.nlp_engineering.embedding_generator.cpuembeddinggenerator attribute)": [[2, "bardi.nlp_engineering.embedding_generator.CPUEmbeddingGenerator.min_alpha"]], "min_alpha (bardi.nlp_engineering.embedding_generator.embeddinggenerator attribute)": [[2, "bardi.nlp_engineering.embedding_generator.EmbeddingGenerator.min_alpha"]], "min_word_count (bardi.nlp_engineering.embedding_generator.cpuembeddinggenerator attribute)": [[2, "bardi.nlp_engineering.embedding_generator.CPUEmbeddingGenerator.min_word_count"]], "min_word_count (bardi.nlp_engineering.embedding_generator.embeddinggenerator attribute)": [[2, "bardi.nlp_engineering.embedding_generator.EmbeddingGenerator.min_word_count"]], "model_name (bardi.nlp_engineering.tokenizer_encoder.cputokenizerencoder attribute)": [[2, "bardi.nlp_engineering.tokenizer_encoder.CPUTokenizerEncoder.model_name"]], "model_name (bardi.nlp_engineering.tokenizer_encoder.tokenizerencoder attribute)": [[2, "bardi.nlp_engineering.tokenizer_encoder.TokenizerEncoder.model_name"]], "negative (bardi.nlp_engineering.embedding_generator.cpuembeddinggenerator attribute)": [[2, "bardi.nlp_engineering.embedding_generator.CPUEmbeddingGenerator.negative"]], "negative (bardi.nlp_engineering.embedding_generator.embeddinggenerator attribute)": [[2, "bardi.nlp_engineering.embedding_generator.EmbeddingGenerator.negative"]], "no_substitution (bardi.nlp_engineering.regex_library.regex_set.regexset attribute)": [[2, "bardi.nlp_engineering.regex_library.regex_set.RegexSet.no_substitution"]], "num_splits (bardi.nlp_engineering.splitter.cpusplitter attribute)": [[2, "bardi.nlp_engineering.splitter.CPUSplitter.num_splits"]], "num_splits (bardi.nlp_engineering.splitter.splitter attribute)": [[2, "bardi.nlp_engineering.splitter.Splitter.num_splits"]], "random_seed (bardi.nlp_engineering.splitter.cpusplitter attribute)": [[2, "bardi.nlp_engineering.splitter.CPUSplitter.random_seed"]], "random_seed (bardi.nlp_engineering.splitter.newsplit attribute)": [[2, "bardi.nlp_engineering.splitter.NewSplit.random_seed"]], "random_seed (bardi.nlp_engineering.splitter.splitter attribute)": [[2, "bardi.nlp_engineering.splitter.Splitter.random_seed"]], "regex_set (bardi.nlp_engineering.normalizer.cpunormalizer attribute)": [[2, "bardi.nlp_engineering.normalizer.CPUNormalizer.regex_set"]], "regex_set (bardi.nlp_engineering.normalizer.normalizer attribute)": [[2, "bardi.nlp_engineering.normalizer.Normalizer.regex_set"]], "regex_set (bardi.nlp_engineering.regex_library.regex_set.regexset attribute)": [[2, "bardi.nlp_engineering.regex_library.regex_set.RegexSet.regex_set"]], "regex_str (bardi.nlp_engineering.regex_library.regex_set.regexsubpair attribute)": [[2, "bardi.nlp_engineering.regex_library.regex_set.RegexSubPair.regex_str"], [2, "id1"]], "remove_addresses (bardi.nlp_engineering.regex_library.pathology_report.pathologyreportregexset attribute)": [[2, "bardi.nlp_engineering.regex_library.pathology_report.PathologyReportRegexSet.remove_addresses"]], "remove_cassette_names (bardi.nlp_engineering.regex_library.pathology_report.pathologyreportregexset attribute)": [[2, "bardi.nlp_engineering.regex_library.pathology_report.PathologyReportRegexSet.remove_cassette_names"]], "remove_dates (bardi.nlp_engineering.regex_library.pathology_report.pathologyreportregexset attribute)": [[2, "bardi.nlp_engineering.regex_library.pathology_report.PathologyReportRegexSet.remove_dates"]], "remove_decimal_seg_numbers (bardi.nlp_engineering.regex_library.pathology_report.pathologyreportregexset attribute)": [[2, "bardi.nlp_engineering.regex_library.pathology_report.PathologyReportRegexSet.remove_decimal_seg_numbers"]], "remove_dimensions (bardi.nlp_engineering.regex_library.pathology_report.pathologyreportregexset attribute)": [[2, "bardi.nlp_engineering.regex_library.pathology_report.PathologyReportRegexSet.remove_dimensions"]], "remove_duration_time (bardi.nlp_engineering.regex_library.pathology_report.pathologyreportregexset attribute)": [[2, "bardi.nlp_engineering.regex_library.pathology_report.PathologyReportRegexSet.remove_duration_time"]], "remove_large_digits_seq (bardi.nlp_engineering.regex_library.pathology_report.pathologyreportregexset attribute)": [[2, "bardi.nlp_engineering.regex_library.pathology_report.PathologyReportRegexSet.remove_large_digits_seq"]], "remove_large_floats_seq (bardi.nlp_engineering.regex_library.pathology_report.pathologyreportregexset attribute)": [[2, "bardi.nlp_engineering.regex_library.pathology_report.PathologyReportRegexSet.remove_large_floats_seq"]], "remove_leading_punct (bardi.nlp_engineering.regex_library.pathology_report.pathologyreportregexset attribute)": [[2, "bardi.nlp_engineering.regex_library.pathology_report.PathologyReportRegexSet.remove_leading_punct"]], "remove_letter_num_seq (bardi.nlp_engineering.regex_library.pathology_report.pathologyreportregexset attribute)": [[2, "bardi.nlp_engineering.regex_library.pathology_report.PathologyReportRegexSet.remove_letter_num_seq"]], "remove_multiple_punct (bardi.nlp_engineering.regex_library.pathology_report.pathologyreportregexset attribute)": [[2, "bardi.nlp_engineering.regex_library.pathology_report.PathologyReportRegexSet.remove_multiple_punct"]], "remove_phone_numbers (bardi.nlp_engineering.regex_library.pathology_report.pathologyreportregexset attribute)": [[2, "bardi.nlp_engineering.regex_library.pathology_report.PathologyReportRegexSet.remove_phone_numbers"]], "remove_special_punct (bardi.nlp_engineering.regex_library.pathology_report.pathologyreportregexset attribute)": [[2, "bardi.nlp_engineering.regex_library.pathology_report.PathologyReportRegexSet.remove_special_punct"]], "remove_specimen (bardi.nlp_engineering.regex_library.pathology_report.pathologyreportregexset attribute)": [[2, "bardi.nlp_engineering.regex_library.pathology_report.PathologyReportRegexSet.remove_specimen"]], "remove_times (bardi.nlp_engineering.regex_library.pathology_report.pathologyreportregexset attribute)": [[2, "bardi.nlp_engineering.regex_library.pathology_report.PathologyReportRegexSet.remove_times"]], "remove_trailing_punct (bardi.nlp_engineering.regex_library.pathology_report.pathologyreportregexset attribute)": [[2, "bardi.nlp_engineering.regex_library.pathology_report.PathologyReportRegexSet.remove_trailing_punct"]], "remove_urls (bardi.nlp_engineering.regex_library.pathology_report.pathologyreportregexset attribute)": [[2, "bardi.nlp_engineering.regex_library.pathology_report.PathologyReportRegexSet.remove_urls"]], "replace_percent_sign (bardi.nlp_engineering.regex_library.pathology_report.pathologyreportregexset attribute)": [[2, "bardi.nlp_engineering.regex_library.pathology_report.PathologyReportRegexSet.replace_percent_sign"]], "retain_concat_field (bardi.nlp_engineering.tokenizer_encoder.cputokenizerencoder attribute)": [[2, "bardi.nlp_engineering.tokenizer_encoder.CPUTokenizerEncoder.retain_concat_field"]], "retain_concat_field (bardi.nlp_engineering.tokenizer_encoder.tokenizerencoder attribute)": [[2, "bardi.nlp_engineering.tokenizer_encoder.TokenizerEncoder.retain_concat_field"]], "retain_input_fields (bardi.nlp_engineering.label_processor.cpulabelprocessor attribute)": [[2, "bardi.nlp_engineering.label_processor.CPULabelProcessor.retain_input_fields"]], "retain_input_fields (bardi.nlp_engineering.label_processor.labelprocessor attribute)": [[2, "bardi.nlp_engineering.label_processor.LabelProcessor.retain_input_fields"]], "retain_input_fields (bardi.nlp_engineering.normalizer.cpunormalizer attribute)": [[2, "bardi.nlp_engineering.normalizer.CPUNormalizer.retain_input_fields"]], "retain_input_fields (bardi.nlp_engineering.normalizer.normalizer attribute)": [[2, "bardi.nlp_engineering.normalizer.Normalizer.retain_input_fields"]], "retain_input_fields (bardi.nlp_engineering.pre_tokenizer.cpupretokenizer attribute)": [[2, "bardi.nlp_engineering.pre_tokenizer.CPUPreTokenizer.retain_input_fields"]], "retain_input_fields (bardi.nlp_engineering.pre_tokenizer.pretokenizer attribute)": [[2, "bardi.nlp_engineering.pre_tokenizer.PreTokenizer.retain_input_fields"]], "retain_input_fields (bardi.nlp_engineering.tokenizer_encoder.cputokenizerencoder attribute)": [[2, "bardi.nlp_engineering.tokenizer_encoder.CPUTokenizerEncoder.retain_input_fields"]], "retain_input_fields (bardi.nlp_engineering.tokenizer_encoder.tokenizerencoder attribute)": [[2, "bardi.nlp_engineering.tokenizer_encoder.TokenizerEncoder.retain_input_fields"]], "retain_input_fields (bardi.nlp_engineering.vocab_encoder.cpuvocabencoder attribute)": [[2, "bardi.nlp_engineering.vocab_encoder.CPUVocabEncoder.retain_input_fields"]], "retain_input_fields (bardi.nlp_engineering.vocab_encoder.vocabencoder attribute)": [[2, "bardi.nlp_engineering.vocab_encoder.VocabEncoder.retain_input_fields"]], "return_tensors (bardi.nlp_engineering.tokenizer_encoder.cputokenizerencoder attribute)": [[2, "bardi.nlp_engineering.tokenizer_encoder.CPUTokenizerEncoder.return_tensors"]], "return_tensors (bardi.nlp_engineering.tokenizer_encoder.tokenizerencoder attribute)": [[2, "bardi.nlp_engineering.tokenizer_encoder.TokenizerEncoder.return_tensors"]], "run() (bardi.nlp_engineering.embedding_generator.cpuembeddinggenerator method)": [[2, "bardi.nlp_engineering.embedding_generator.CPUEmbeddingGenerator.run"]], "run() (bardi.nlp_engineering.embedding_generator.embeddinggenerator method)": [[2, "bardi.nlp_engineering.embedding_generator.EmbeddingGenerator.run"]], "run() (bardi.nlp_engineering.label_processor.cpulabelprocessor method)": [[2, "bardi.nlp_engineering.label_processor.CPULabelProcessor.run"]], "run() (bardi.nlp_engineering.label_processor.labelprocessor method)": [[2, "bardi.nlp_engineering.label_processor.LabelProcessor.run"]], "run() (bardi.nlp_engineering.normalizer.cpunormalizer method)": [[2, "bardi.nlp_engineering.normalizer.CPUNormalizer.run"]], "run() (bardi.nlp_engineering.normalizer.normalizer method)": [[2, "bardi.nlp_engineering.normalizer.Normalizer.run"]], "run() (bardi.nlp_engineering.pre_tokenizer.cpupretokenizer method)": [[2, "bardi.nlp_engineering.pre_tokenizer.CPUPreTokenizer.run"]], "run() (bardi.nlp_engineering.pre_tokenizer.pretokenizer method)": [[2, "bardi.nlp_engineering.pre_tokenizer.PreTokenizer.run"]], "run() (bardi.nlp_engineering.splitter.cpusplitter method)": [[2, "bardi.nlp_engineering.splitter.CPUSplitter.run"]], "run() (bardi.nlp_engineering.splitter.splitter method)": [[2, "bardi.nlp_engineering.splitter.Splitter.run"]], "run() (bardi.nlp_engineering.tokenizer_encoder.cputokenizerencoder method)": [[2, "bardi.nlp_engineering.tokenizer_encoder.CPUTokenizerEncoder.run"]], "run() (bardi.nlp_engineering.tokenizer_encoder.tokenizerencoder method)": [[2, "bardi.nlp_engineering.tokenizer_encoder.TokenizerEncoder.run"]], "run() (bardi.nlp_engineering.tokenizer_trainer.cputokenizertrainer method)": [[2, "bardi.nlp_engineering.tokenizer_trainer.CPUTokenizerTrainer.run"]], "run() (bardi.nlp_engineering.tokenizer_trainer.tokenizertrainer method)": [[2, "bardi.nlp_engineering.tokenizer_trainer.TokenizerTrainer.run"]], "run() (bardi.nlp_engineering.vocab_encoder.cpuvocabencoder method)": [[2, "bardi.nlp_engineering.vocab_encoder.CPUVocabEncoder.run"]], "run() (bardi.nlp_engineering.vocab_encoder.vocabencoder method)": [[2, "bardi.nlp_engineering.vocab_encoder.VocabEncoder.run"]], "sample (bardi.nlp_engineering.embedding_generator.cpuembeddinggenerator attribute)": [[2, "bardi.nlp_engineering.embedding_generator.CPUEmbeddingGenerator.sample"]], "sample (bardi.nlp_engineering.embedding_generator.embeddinggenerator attribute)": [[2, "bardi.nlp_engineering.embedding_generator.EmbeddingGenerator.sample"]], "seed (bardi.nlp_engineering.embedding_generator.cpuembeddinggenerator attribute)": [[2, "bardi.nlp_engineering.embedding_generator.CPUEmbeddingGenerator.seed"]], "seed (bardi.nlp_engineering.embedding_generator.embeddinggenerator attribute)": [[2, "bardi.nlp_engineering.embedding_generator.EmbeddingGenerator.seed"]], "set_write_config() (bardi.nlp_engineering.embedding_generator.embeddinggenerator method)": [[2, "bardi.nlp_engineering.embedding_generator.EmbeddingGenerator.set_write_config"]], "set_write_config() (bardi.nlp_engineering.label_processor.labelprocessor method)": [[2, "bardi.nlp_engineering.label_processor.LabelProcessor.set_write_config"]], "set_write_config() (bardi.nlp_engineering.tokenizer_trainer.tokenizertrainer method)": [[2, "bardi.nlp_engineering.tokenizer_trainer.TokenizerTrainer.set_write_config"]], "split_mapping (bardi.nlp_engineering.splitter.cpusplitter attribute)": [[2, "bardi.nlp_engineering.splitter.CPUSplitter.split_mapping"]], "split_mapping (bardi.nlp_engineering.splitter.mapsplit attribute)": [[2, "bardi.nlp_engineering.splitter.MapSplit.split_mapping"]], "split_mapping (bardi.nlp_engineering.splitter.splitter attribute)": [[2, "bardi.nlp_engineering.splitter.Splitter.split_mapping"]], "split_method (bardi.nlp_engineering.splitter.cpusplitter attribute)": [[2, "bardi.nlp_engineering.splitter.CPUSplitter.split_method"]], "split_method (bardi.nlp_engineering.splitter.splitter attribute)": [[2, "bardi.nlp_engineering.splitter.Splitter.split_method"]], "split_pattern (bardi.nlp_engineering.pre_tokenizer.cpupretokenizer attribute)": [[2, "bardi.nlp_engineering.pre_tokenizer.CPUPreTokenizer.split_pattern"]], "split_pattern (bardi.nlp_engineering.pre_tokenizer.pretokenizer attribute)": [[2, "bardi.nlp_engineering.pre_tokenizer.PreTokenizer.split_pattern"]], "split_proportions (bardi.nlp_engineering.splitter.cpusplitter attribute)": [[2, "bardi.nlp_engineering.splitter.CPUSplitter.split_proportions"]], "split_proportions (bardi.nlp_engineering.splitter.newsplit attribute)": [[2, "bardi.nlp_engineering.splitter.NewSplit.split_proportions"]], "split_proportions (bardi.nlp_engineering.splitter.splitter attribute)": [[2, "bardi.nlp_engineering.splitter.Splitter.split_proportions"]], "split_type (bardi.nlp_engineering.splitter.cpusplitter attribute)": [[2, "bardi.nlp_engineering.splitter.CPUSplitter.split_type"]], "split_type (bardi.nlp_engineering.splitter.splitter attribute)": [[2, "bardi.nlp_engineering.splitter.Splitter.split_type"]], "sub_str (bardi.nlp_engineering.regex_library.regex_set.regexsubpair attribute)": [[2, "bardi.nlp_engineering.regex_library.regex_set.RegexSubPair.sub_str"], [2, "id2"]], "tokenizer_fname (bardi.nlp_engineering.tokenizer_trainer.cputokenizertrainer attribute)": [[2, "bardi.nlp_engineering.tokenizer_trainer.CPUTokenizerTrainer.tokenizer_fname"]], "tokenizer_fname (bardi.nlp_engineering.tokenizer_trainer.tokenizertrainer attribute)": [[2, "bardi.nlp_engineering.tokenizer_trainer.TokenizerTrainer.tokenizer_fname"]], "tokenizer_model (bardi.nlp_engineering.tokenizer_encoder.cputokenizerencoder attribute)": [[2, "bardi.nlp_engineering.tokenizer_encoder.CPUTokenizerEncoder.tokenizer_model"]], "tokenizer_model (bardi.nlp_engineering.tokenizer_encoder.tokenizerencoder attribute)": [[2, "bardi.nlp_engineering.tokenizer_encoder.TokenizerEncoder.tokenizer_model"]], "tokenizer_params (bardi.nlp_engineering.tokenizer_encoder.cputokenizerencoder attribute)": [[2, "bardi.nlp_engineering.tokenizer_encoder.CPUTokenizerEncoder.tokenizer_params"]], "tokenizer_params (bardi.nlp_engineering.tokenizer_encoder.tokenizerencoder attribute)": [[2, "bardi.nlp_engineering.tokenizer_encoder.TokenizerEncoder.tokenizer_params"]], "tokenizer_type (bardi.nlp_engineering.tokenizer_trainer.cputokenizertrainer attribute)": [[2, "bardi.nlp_engineering.tokenizer_trainer.CPUTokenizerTrainer.tokenizer_type"]], "tokenizer_type (bardi.nlp_engineering.tokenizer_trainer.tokenizertrainer attribute)": [[2, "bardi.nlp_engineering.tokenizer_trainer.TokenizerTrainer.tokenizer_type"]], "trunc_decimals (bardi.nlp_engineering.regex_library.pathology_report.pathologyreportregexset attribute)": [[2, "bardi.nlp_engineering.regex_library.pathology_report.PathologyReportRegexSet.trunc_decimals"]], "unique_record_cols (bardi.nlp_engineering.splitter.cpusplitter attribute)": [[2, "bardi.nlp_engineering.splitter.CPUSplitter.unique_record_cols"]], "unique_record_cols (bardi.nlp_engineering.splitter.mapsplit attribute)": [[2, "bardi.nlp_engineering.splitter.MapSplit.unique_record_cols"]], "unique_record_cols (bardi.nlp_engineering.splitter.newsplit attribute)": [[2, "bardi.nlp_engineering.splitter.NewSplit.unique_record_cols"]], "unique_record_cols (bardi.nlp_engineering.splitter.splitter attribute)": [[2, "bardi.nlp_engineering.splitter.Splitter.unique_record_cols"]], "vector_size (bardi.nlp_engineering.embedding_generator.cpuembeddinggenerator attribute)": [[2, "bardi.nlp_engineering.embedding_generator.CPUEmbeddingGenerator.vector_size"]], "vector_size (bardi.nlp_engineering.embedding_generator.embeddinggenerator attribute)": [[2, "bardi.nlp_engineering.embedding_generator.EmbeddingGenerator.vector_size"]], "vocab_exclude_list (bardi.nlp_engineering.embedding_generator.cpuembeddinggenerator attribute)": [[2, "bardi.nlp_engineering.embedding_generator.CPUEmbeddingGenerator.vocab_exclude_list"]], "vocab_exclude_list (bardi.nlp_engineering.embedding_generator.embeddinggenerator attribute)": [[2, "bardi.nlp_engineering.embedding_generator.EmbeddingGenerator.vocab_exclude_list"]], "vocab_format (bardi.nlp_engineering.embedding_generator.embeddinggeneratorartifactswriteconfig attribute)": [[2, "bardi.nlp_engineering.embedding_generator.EmbeddingGeneratorArtifactsWriteConfig.vocab_format"]], "vocab_format (bardi.nlp_engineering.tokenizer_trainer.tokenizertrainerartifactswriteconfig attribute)": [[2, "bardi.nlp_engineering.tokenizer_trainer.TokenizerTrainerArtifactsWriteConfig.vocab_format"]], "vocab_format_args (bardi.nlp_engineering.embedding_generator.embeddinggeneratorartifactswriteconfig attribute)": [[2, "bardi.nlp_engineering.embedding_generator.EmbeddingGeneratorArtifactsWriteConfig.vocab_format_args"]], "vocab_format_args (bardi.nlp_engineering.tokenizer_trainer.tokenizertrainerartifactswriteconfig attribute)": [[2, "bardi.nlp_engineering.tokenizer_trainer.TokenizerTrainerArtifactsWriteConfig.vocab_format_args"]], "vocab_size (bardi.nlp_engineering.tokenizer_trainer.cputokenizertrainer attribute)": [[2, "bardi.nlp_engineering.tokenizer_trainer.CPUTokenizerTrainer.vocab_size"]], "vocab_size (bardi.nlp_engineering.tokenizer_trainer.tokenizertrainer attribute)": [[2, "bardi.nlp_engineering.tokenizer_trainer.TokenizerTrainer.vocab_size"]], "window (bardi.nlp_engineering.embedding_generator.cpuembeddinggenerator attribute)": [[2, "bardi.nlp_engineering.embedding_generator.CPUEmbeddingGenerator.window"]], "window (bardi.nlp_engineering.embedding_generator.embeddinggenerator attribute)": [[2, "bardi.nlp_engineering.embedding_generator.EmbeddingGenerator.window"]], "write_artifacts() (bardi.nlp_engineering.embedding_generator.cpuembeddinggenerator method)": [[2, "bardi.nlp_engineering.embedding_generator.CPUEmbeddingGenerator.write_artifacts"]], "write_artifacts() (bardi.nlp_engineering.label_processor.cpulabelprocessor method)": [[2, "bardi.nlp_engineering.label_processor.CPULabelProcessor.write_artifacts"]], "write_artifacts() (bardi.nlp_engineering.tokenizer_trainer.cputokenizertrainer method)": [[2, "bardi.nlp_engineering.tokenizer_trainer.CPUTokenizerTrainer.write_artifacts"]], "datawriteconfig (class in bardi.pipeline)": [[3, "bardi.pipeline.DataWriteConfig"]], "pipeline (class in bardi.pipeline)": [[3, "bardi.pipeline.Pipeline"]], "step (class in bardi.pipeline)": [[3, "bardi.pipeline.Step"]], "add_step() (bardi.pipeline.pipeline method)": [[3, "bardi.pipeline.Pipeline.add_step"]], "bardi.pipeline": [[3, "module-bardi.pipeline"]], "data_filename (bardi.pipeline.pipeline attribute)": [[3, "bardi.pipeline.Pipeline.data_filename"]], "data_format (bardi.pipeline.datawriteconfig attribute)": [[3, "bardi.pipeline.DataWriteConfig.data_format"]], "data_format_args (bardi.pipeline.datawriteconfig attribute)": [[3, "bardi.pipeline.DataWriteConfig.data_format_args"]], "data_write_config (bardi.pipeline.pipeline attribute)": [[3, "bardi.pipeline.Pipeline.data_write_config"]], "dataset (bardi.pipeline.pipeline attribute)": [[3, "bardi.pipeline.Pipeline.dataset"]], "get_parameters() (bardi.pipeline.pipeline method)": [[3, "bardi.pipeline.Pipeline.get_parameters"]], "get_parameters() (bardi.pipeline.step method)": [[3, "bardi.pipeline.Step.get_parameters"]], "run() (bardi.pipeline.step method)": [[3, "bardi.pipeline.Step.run"]], "run_pipeline() (bardi.pipeline.pipeline method)": [[3, "bardi.pipeline.Pipeline.run_pipeline"]], "set_write_config() (bardi.pipeline.step method)": [[3, "bardi.pipeline.Step.set_write_config"]], "write_artifacts() (bardi.pipeline.step method)": [[3, "bardi.pipeline.Step.write_artifacts"]], "write_data() (bardi.pipeline.step method)": [[3, "bardi.pipeline.Step.write_data"]], "write_outputs (bardi.pipeline.pipeline attribute)": [[3, "bardi.pipeline.Pipeline.write_outputs"]], "write_path (bardi.pipeline.pipeline attribute)": [[3, "bardi.pipeline.Pipeline.write_path"]]}})